from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import VectorIndexRetriever, QueryFusionRetriever, BaseRetriever
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch import nn
import torch
import copy
from .utils import clear_gpu_cache
from .config import RERANKER_MODEL, MAX_LENGTH, BATCH_SIZE

from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.schema import NodeWithScore, QueryBundle, MetadataMode
from llama_index.core.bridge.pydantic import Field, PrivateAttr
from llama_index.core.callbacks import CBEventType, EventPayload
from typing import List, Optional, Any, Union
from pathlib import Path
import torch
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM

from typing import List, Optional, Any, Dict, Tuple
import math


from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.schema import NodeWithScore, QueryBundle, MetadataMode
from llama_index.core.bridge.pydantic import Field, PrivateAttr
from llama_index.core.callbacks.base import CallbackManager
from llama_index.core.instrumentation import get_dispatcher
from llama_index.core.instrumentation.events.rerank import (
    ReRankEndEvent,
    ReRankStartEvent,
)
from llama_index.core.callbacks.schema import CBEventType, EventPayload

from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from vllm.inputs import TokensPrompt  # æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
from vllm.distributed.parallel_state import destroy_model_parallel
import time
from typing import Dict
import logging

# åŸºæœ¬é…ç½®
logging.basicConfig(level=logging.INFO)
log = logging.info  # æˆ–è€…ä½¿ç”¨ logger

class Qwen3Reranker(BaseNodePostprocessor):
    """
    Qwen3-Reranker for reranking nodes based on relevance to query.
    
    Args:
        model (str): Path to the Qwen3-Reranker model.
        top_n (int): Number of nodes to return sorted by score. Defaults to 5.
        device (str, optional): Device (like "cuda", "cpu") for computation. 
            If None, checks if a GPU can be used.
        max_length (int): Maximum sequence length. Defaults to 8192.
        instruction (str, optional): Custom instruction for reranking.
        keep_retrieval_score (bool, optional): Whether to keep the retrieval score 
            in metadata. Defaults to False.
        batch_size (int): Number of query-document pairs to process at once.
            Defaults to 5. Lower values use less memory but may be slower.
        use_kv_cache (bool): Whether to use KV cache for acceleration.
            Defaults to True.
        max_kv_cache_size (int): Maximum number of batches to keep in KV cache.
            Defaults to 3. Set to 0 to disable cache limit.
        enable_gradient_checkpointing (bool): Whether to use gradient checkpointing
            to save memory. Defaults to False.
    """
    
    model: str = Field(description="Path to Qwen3-Reranker model.")
    top_n: int = Field(default=5, description="Number of nodes to return sorted by score.")
    device: Optional[str] = Field(default=None, description="Device for computation.")
    max_length: int = Field(default=8192, description="Maximum sequence length.")
    instruction: Optional[str] = Field(
        default=None, 
        description="Custom instruction for reranking."
    )
    keep_retrieval_score: bool = Field(
        default=False,
        description="Whether to keep the retrieval score in metadata.",
    )
    batch_size: int = Field(
        default=5,
        description="Number of query-document pairs to process at once.",
    )
    use_kv_cache: bool = Field(
        default=True,
        description="Whether to use KV cache for acceleration.",
    )
    max_kv_cache_size: int = Field(
        default=3,
        description="Maximum number of batches to keep in KV cache. 0 means no limit.",
    )
    enable_gradient_checkpointing: bool = Field(
        default=False,
        description="Whether to use gradient checkpointing to save memory.",
    )
    
    # ç§æœ‰å±æ€§
    _tokenizer: Any = PrivateAttr()
    _model: Any = PrivateAttr()
    _device: str = PrivateAttr()
    _token_false_id: int = PrivateAttr()
    _token_true_id: int = PrivateAttr()
    _prefix: str = PrivateAttr()
    _suffix: str = PrivateAttr()
    _prefix_tokens: List[int] = PrivateAttr()
    _suffix_tokens: List[int] = PrivateAttr()
    _kv_cache_counter: int = PrivateAttr(default=0)
    
    def __init__(
        self,
        model: str,
        top_n: int = 5,
        device: Optional[str] = None,
        max_length: int = 8192,
        instruction: Optional[str] = None,
        keep_retrieval_score: bool = False,
        batch_size: int = 5,
        use_kv_cache: bool = True,
        max_kv_cache_size: int = 3,
        enable_gradient_checkpointing: bool = False,
        **kwargs
    ):
        # å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä¼ é€’æ‰€æœ‰ Field å±æ€§
        super().__init__(
            model=model,
            top_n=top_n,
            device=device,
            max_length=max_length,
            instruction=instruction,
            keep_retrieval_score=keep_retrieval_score,
            batch_size=batch_size,
            use_kv_cache=use_kv_cache,
            max_kv_cache_size=max_kv_cache_size,
            enable_gradient_checkpointing=enable_gradient_checkpointing,
            **kwargs
        )
        
        # éªŒè¯ batch_size
        if self.batch_size < 1:
            raise ValueError(f"batch_size must be >= 1, got {self.batch_size}")
        
        # è®¾ç½®é»˜è®¤ instruction
        if self.instruction is None:
            self.instruction = "Given a web search query, retrieve relevant passages that answer the query"
        
        # æ¨æ–­è®¾å¤‡
        if self.device is None:
            self._device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self._device = self.device
        
        # åŠ è½½ tokenizer
        self._tokenizer = AutoTokenizer.from_pretrained(
            self.model, 
            padding_side='left'
        )
        
        # åŠ è½½æ¨¡å‹
        try:
            self._model = AutoModelForCausalLM.from_pretrained(
                self.model,
                torch_dtype=torch.float16,  # ä½¿ç”¨ float16 å‡å°‘æ˜¾å­˜
                attn_implementation="flash_attention_2",
                use_cache=self.use_kv_cache  # æ ¹æ®é…ç½®å¯ç”¨ KV cache
            ).to(self._device).eval()
            log("âœ“ Using flash_attention_2")
        except Exception as e:
            log(f"âš  Flash attention not available, using default: {e}")
            self._model = AutoModelForCausalLM.from_pretrained(
                self.model,
                torch_dtype=torch.float16,
                use_cache=self.use_kv_cache
            ).to(self._device).eval()
        
        # å¯ç”¨ gradient checkpointing ä»¥èŠ‚çœæ˜¾å­˜
        if self.enable_gradient_checkpointing:
            self._model.gradient_checkpointing_enable()
            log("âœ“ Gradient checkpointing enabled")
        
        # è·å– yes/no token ids
        self._token_false_id = self._tokenizer.convert_tokens_to_ids("no")
        self._token_true_id = self._tokenizer.convert_tokens_to_ids("yes")
        
        # å®šä¹‰å‰ç¼€å’Œåç¼€
        self._prefix = (
            "<|im_start|>system\n"
            "Judge whether the Document meets the requirements based on the Query and the Instruct provided. "
            "Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n"
            "<|im_start|>user\n"
        )
        self._suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        self._prefix_tokens = self._tokenizer.encode(self._prefix, add_special_tokens=False)
        self._suffix_tokens = self._tokenizer.encode(self._suffix, add_special_tokens=False)
        
        # åˆå§‹åŒ– KV cache è®¡æ•°å™¨
        self._kv_cache_counter = 0
    
    @classmethod
    def class_name(cls) -> str:
        """è¿”å›ç±»å,ç”¨äºåºåˆ—åŒ–"""
        return "Qwen3Reranker"
    
    def _selective_cache_clear(self):
        """
        é€‰æ‹©æ€§æ¸…ç†ç¼“å­˜:
        - åªæ¸…ç†è¾“å…¥å¼ é‡,ä¸æ¸…ç†æ¨¡å‹çš„ KV cache
        - å®šæœŸæ¸…ç†ä»¥é˜²æ­¢æ˜¾å­˜ç´¯ç§¯
        """
        if self._device.startswith("cuda"):
            # åªæ¸…ç©ºæœªä½¿ç”¨çš„ç¼“å­˜,ä¸å½±å“å·²åˆ†é…çš„ KV cache
            torch.cuda.empty_cache()
            
            # æ¯å¤„ç† max_kv_cache_size ä¸ªæ‰¹æ¬¡å,è¿›è¡Œä¸€æ¬¡å®Œæ•´æ¸…ç†
            self._kv_cache_counter += 1
            if self.max_kv_cache_size > 0 and self._kv_cache_counter >= self.max_kv_cache_size:
                # æ¸…ç†æ¨¡å‹çš„ past_key_values (KV cache)
                if hasattr(self._model, 'clear_cache'):
                    self._model.clear_cache()
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                torch.cuda.synchronize()
                
                # é‡ç½®è®¡æ•°å™¨
                self._kv_cache_counter = 0
                log(f"âœ“ Periodic cache clear after {self.max_kv_cache_size} batches")
    
    def _format_instruction(self, query: str, doc: str) -> str:
        """æ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬"""
        return f"<Instruct>: {self.instruction}\n<Query>: {query}\n<Document>: {doc}"
    
    def _process_inputs(self, pairs: List[str]):
        """å¤„ç†è¾“å…¥å¯¹"""
        inputs = self._tokenizer(
            pairs, 
            padding=False, 
            truncation='longest_first',
            return_attention_mask=False, 
            max_length=self.max_length - len(self._prefix_tokens) - len(self._suffix_tokens)
        )
        
        # æ·»åŠ å‰ç¼€å’Œåç¼€
        for i, ele in enumerate(inputs['input_ids']):
            inputs['input_ids'][i] = self._prefix_tokens + ele + self._suffix_tokens
        
        # å¡«å……
        inputs = self._tokenizer.pad(
            inputs, 
            padding=True, 
            return_tensors="pt", 
            max_length=self.max_length
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        for key in inputs:
            inputs[key] = inputs[key].to(self._device)
        
        return inputs
    
    @torch.no_grad()
    def _compute_scores_batch(self, pairs: List[str], past_key_values=None) -> tuple:
        """
        è®¡ç®—ä¸€æ‰¹ pairs çš„ç›¸å…³æ€§åˆ†æ•°,æ”¯æŒ KV cache
        
        Args:
            pairs: æ ¼å¼åŒ–åçš„ query-document å¯¹åˆ—è¡¨
            past_key_values: ä¸Šä¸€æ‰¹æ¬¡çš„ KV cache (å¦‚æœå¯ç”¨)
            
        Returns:
            (scores, new_past_key_values) å…ƒç»„
        """
        inputs = self._process_inputs(pairs)
        
        try:
            # å¦‚æœä½¿ç”¨ KV cache ä¸”æœ‰ past_key_values,ä¼ å…¥æ¨¡å‹
            model_kwargs = {}
            if self.use_kv_cache and past_key_values is not None:
                model_kwargs['past_key_values'] = past_key_values
                model_kwargs['use_cache'] = True
            
            outputs = self._model(**inputs, **model_kwargs)
            batch_scores = outputs.logits[:, -1, :]
            
            true_vector = batch_scores[:, self._token_true_id]
            false_vector = batch_scores[:, self._token_false_id]
            batch_scores = torch.stack([false_vector, true_vector], dim=1)
            batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
            scores = batch_scores[:, 1].exp().tolist()
            
            # è¿”å›åˆ†æ•°å’Œæ–°çš„ KV cache
            new_past_key_values = outputs.past_key_values if self.use_kv_cache else None
            
            return scores, new_past_key_values
            
        finally:
            # åªåˆ é™¤è¾“å…¥å¼ é‡,ä¿ç•™ KV cache
            del inputs
            # é€‰æ‹©æ€§æ¸…ç†ç¼“å­˜
            self._selective_cache_clear()
    
    def _compute_scores(self, pairs: List[str]) -> List[float]:
        """
        åˆ†æ‰¹è®¡ç®—æ‰€æœ‰ pairs çš„ç›¸å…³æ€§åˆ†æ•°,ä½¿ç”¨ KV cache åŠ é€Ÿ
        
        Args:
            pairs: æ ¼å¼åŒ–åçš„ query-document å¯¹åˆ—è¡¨
            
        Returns:
            æ‰€æœ‰ pairs çš„åˆ†æ•°åˆ—è¡¨
        """
        all_scores = []
        total_pairs = len(pairs)
        past_key_values = None  # ç”¨äºå­˜å‚¨ KV cache
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total_pairs, self.batch_size):
            batch_pairs = pairs[i:i + self.batch_size]
            
            # æ³¨æ„:å¯¹äº reranker,æ¯ä¸ª batch çš„ query-doc å¯¹æ˜¯ç‹¬ç«‹çš„
            # æ‰€ä»¥è¿™é‡Œä¸ä¼ é€’ past_key_values,æ¯ä¸ª batch ç‹¬ç«‹è®¡ç®—
            # å¦‚æœä½ çš„åœºæ™¯æ˜¯åŒä¸€ä¸ª query å¯¹å¤šä¸ª doc,å¯ä»¥å¤ç”¨ query çš„ KV cache
            batch_scores, _ = self._compute_scores_batch(batch_pairs, past_key_values=None)
            all_scores.extend(batch_scores)
            
            # æ‰“å°è¿›åº¦
            if total_pairs > self.batch_size:
                processed = min(i + self.batch_size, total_pairs)
                log(f"Reranking progress: {processed}/{total_pairs} pairs processed")
        
        return all_scores
    
    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -> List[NodeWithScore]:
        """
        é‡æ’åºèŠ‚ç‚¹(å¿…é¡»å®ç°çš„æŠ½è±¡æ–¹æ³•)
        
        Args:
            nodes: å¾…é‡æ’åºçš„èŠ‚ç‚¹åˆ—è¡¨
            query_bundle: æŸ¥è¯¢ä¿¡æ¯
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        if query_bundle is None:
            raise ValueError("Missing query bundle in extra info.")
        
        if len(nodes) == 0:
            return []
        
        # é‡ç½® KV cache è®¡æ•°å™¨(æ–°çš„æŸ¥è¯¢å¼€å§‹)
        self._kv_cache_counter = 0
        
        try:
            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            query_str = query_bundle.query_str
            query_and_nodes = [
                (
                    query_str,
                    node.node.get_content(metadata_mode=MetadataMode.EMBED),
                )
                for node in nodes
            ]
            
            # æ ¼å¼åŒ–è¾“å…¥
            pairs = [
                self._format_instruction(query, doc) 
                for query, doc in query_and_nodes
            ]
            
            # ä½¿ç”¨ callback manager è®°å½•äº‹ä»¶
            with self.callback_manager.event(
                CBEventType.RERANKING,
                payload={
                    EventPayload.NODES: nodes,
                    EventPayload.MODEL_NAME: self.model,
                    EventPayload.QUERY_STR: query_str,
                    EventPayload.TOP_K: self.top_n,
                },
            ) as event:
                # åˆ†æ‰¹å¤„ç†å¹¶è®¡ç®—åˆ†æ•°
                scores = self._compute_scores(pairs)
                
                assert len(scores) == len(nodes), \
                    f"Score count mismatch: got {len(scores)} scores for {len(nodes)} nodes"
                
                # æ›´æ–°èŠ‚ç‚¹åˆ†æ•°
                for node, score in zip(nodes, scores):
                    if self.keep_retrieval_score:
                        node.node.metadata["retrieval_score"] = node.score
                    node.score = float(score)
                
                # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å› top_n
                new_nodes = sorted(
                    nodes, 
                    key=lambda x: -x.score if x.score else 0
                )[: self.top_n]
                
                # è®°å½•ç»“æœ
                event.on_end(payload={EventPayload.NODES: new_nodes})
            
            return new_nodes
        
        finally:
            # å®Œæˆä¸€æ¬¡å®Œæ•´çš„ reranking å,æ¸…ç†ä¸€æ¬¡ç¼“å­˜
            if self._device.startswith("cuda"):
                # æ¸…ç†æ¨¡å‹çš„ KV cache
                if hasattr(self._model, 'clear_cache'):
                    self._model.clear_cache()
                
                torch.cuda.empty_cache()
                gc.collect()
                log("âœ“ Cache cleared after reranking completion")

class Qwen3Reranker_vllm(BaseNodePostprocessor):
    """
    Qwen3-Reranker with vLLM acceleration for high-performance reranking.
    
    Args:
        model (str): Path to the Qwen3-Reranker model. Defaults to "Qwen/Qwen3-Reranker-4B".
        top_n (int): Number of nodes to return sorted by score. Defaults to 5.
        max_length (int): Maximum sequence length. Defaults to 8192.
        instruction (str, optional): Custom instruction for reranking.
        keep_retrieval_score (bool): Whether to keep the retrieval score in metadata. 
            Defaults to False.
        tensor_parallel_size (int, optional): Number of GPUs for tensor parallelism.
            If None, uses all available GPUs.
        gpu_memory_utilization (float): GPU memory utilization ratio. Defaults to 0.8.
        enable_prefix_caching (bool): Whether to enable prefix caching in vLLM.
            Defaults to True for better performance.
        batch_size (int): Number of query-document pairs to process at once.
            Defaults to 32 (vLLM can handle larger batches efficiently).
        max_model_len (int): Maximum model length for vLLM. Defaults to 10000.
    """
    
    model: str = Field(
        default="Qwen/Qwen3-Reranker-4B",
        description="Path to Qwen3-Reranker model."
    )
    top_n: int = Field(
        default=5, 
        description="Number of nodes to return sorted by score."
    )
    max_length: int = Field(
        default=8192, 
        description="Maximum sequence length."
    )
    instruction: Optional[str] = Field(
        default=None, 
        description="Custom instruction for reranking."
    )
    keep_retrieval_score: bool = Field(
        default=False,
        description="Whether to keep the retrieval score in metadata.",
    )
    tensor_parallel_size: Optional[int] = Field(
        default=None,
        description="Number of GPUs for tensor parallelism.",
    )
    gpu_memory_utilization: float = Field(
        default=0.8,
        description="GPU memory utilization ratio.",
    )
    enable_prefix_caching: bool = Field(
        default=True,
        description="Whether to enable prefix caching in vLLM.",
    )
    batch_size: int = Field(
        default=32,
        description="Number of query-document pairs to process at once.",
    )
    max_model_len: int = Field(
        default=10000,
        description="Maximum model length for vLLM.",
    )
    
    # ç§æœ‰å±æ€§
    _tokenizer: Any = PrivateAttr()
    _model: Any = PrivateAttr()
    _sampling_params: Any = PrivateAttr()
    _token_false_id: int = PrivateAttr()
    _token_true_id: int = PrivateAttr()
    _suffix: str = PrivateAttr()
    _suffix_tokens: List[int] = PrivateAttr()
    _system_message: str = PrivateAttr()
    
    def __init__(
        self,
        model: str = "Qwen/Qwen3-Reranker-4B",
        top_n: int = 5,
        max_length: int = 8192,
        instruction: Optional[str] = None,
        keep_retrieval_score: bool = False,
        tensor_parallel_size: Optional[int] = None,
        gpu_memory_utilization: float = 0.8,
        enable_prefix_caching: bool = True,
        batch_size: int = 32,
        max_model_len: int = 10000,
        **kwargs
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            model=model,
            top_n=top_n,
            max_length=max_length,
            instruction=instruction,
            keep_retrieval_score=keep_retrieval_score,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            enable_prefix_caching=enable_prefix_caching,
            batch_size=batch_size,
            max_model_len=max_model_len,
            **kwargs
        )
        
        # éªŒè¯å‚æ•°
        if self.batch_size < 1:
            raise ValueError(f"batch_size must be >= 1, got {self.batch_size}")
        
        if not 0 < self.gpu_memory_utilization <= 1:
            raise ValueError(
                f"gpu_memory_utilization must be in (0, 1], got {self.gpu_memory_utilization}"
            )
        
        # è®¾ç½®é»˜è®¤ instruction
        if self.instruction is None:
            self.instruction = "Given a web search query, retrieve relevant passages that answer the query"
        
        # è®¾ç½®ç³»ç»Ÿæ¶ˆæ¯
        self._system_message = (
            "Judge whether the Document meets the requirements based on the Query and the Instruct provided. "
            "Note that the answer can only be \"yes\" or \"no\"."
        )
        
        # æ¨æ–­ tensor_parallel_size
        if self.tensor_parallel_size is None:
            self.tensor_parallel_size = torch.cuda.device_count()
            if self.tensor_parallel_size == 0:
                self.tensor_parallel_size = 1
                log("âš  No GPU detected, using CPU (will be slow)")
        
        log(f"âœ“ Initializing vLLM with {self.tensor_parallel_size} GPU(s)")
        
        # åŠ è½½ tokenizer
        self._tokenizer = AutoTokenizer.from_pretrained(self.model)
        self._tokenizer.padding_side = "left"
        self._tokenizer.pad_token = self._tokenizer.eos_token
        
        # åˆå§‹åŒ– vLLM æ¨¡å‹
        self._model = LLM(
            model=self.model,
            tensor_parallel_size=self.tensor_parallel_size,
            max_model_len=self.max_model_len,
            enable_prefix_caching=self.enable_prefix_caching,
            gpu_memory_utilization=self.gpu_memory_utilization,
            trust_remote_code=True,
        )
        
        log(f"âœ“ vLLM model loaded successfully")
        log(f"  - Prefix caching: {self.enable_prefix_caching}")
        log(f"  - GPU memory utilization: {self.gpu_memory_utilization}")
        log(f"  - Max model length: {self.max_model_len}")
        
        # è·å– yes/no token ids
        self._token_true_id = self._tokenizer("yes", add_special_tokens=False).input_ids[0]
        self._token_false_id = self._tokenizer("no", add_special_tokens=False).input_ids[0]
        
        # å®šä¹‰åç¼€
        self._suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        self._suffix_tokens = self._tokenizer.encode(self._suffix, add_special_tokens=False)
        
        # é…ç½®é‡‡æ ·å‚æ•°
        self._sampling_params = SamplingParams(
            temperature=0,  # ç¡®å®šæ€§è¾“å‡º
            max_tokens=1,   # åªéœ€è¦ä¸€ä¸ª token (yes/no)
            logprobs=20,    # è¿”å› top-20 logprobs
            allowed_token_ids=[self._token_true_id, self._token_false_id],  # åªå…è®¸ yes/no
        )
        
        log(f"âœ“ Qwen3Reranker_vllm initialized successfully")
    
    @classmethod
    def class_name(cls) -> str:
        """è¿”å›ç±»å,ç”¨äºåºåˆ—åŒ–"""
        return "Qwen3Reranker_vllm"
    
    def _format_instruction(self, query: str, doc: str) -> List[Dict[str, str]]:
        """
        æ ¼å¼åŒ–è¾“å…¥ä¸º chat æ ¼å¼
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            doc: æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            Chat æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        return [
            {"role": "system", "content": self._system_message},
            {
                "role": "user", 
                "content": f"<Instruct>: {self.instruction}\n\n<Query>: {query}\n\n<Document>: {doc}"
            }
        ]
    
    def _process_inputs(self, pairs: List[tuple]) -> List[TokensPrompt]:
        """
        å¤„ç†è¾“å…¥å¯¹,è½¬æ¢ä¸º vLLM æ‰€éœ€æ ¼å¼
        
        Args:
            pairs: (query, doc) å…ƒç»„åˆ—è¡¨
            
        Returns:
            TokensPrompt åˆ—è¡¨
        """
        # æ ¼å¼åŒ–ä¸º chat æ¶ˆæ¯
        messages = [
            self._format_instruction(query, doc) 
            for query, doc in pairs
        ]
        
        # åº”ç”¨ chat template å¹¶ tokenize
        tokenized_messages = self._tokenizer.apply_chat_template(
            messages, 
            tokenize=True, 
            add_generation_prompt=False,
            enable_thinking=False  # Qwen3 ç‰¹æœ‰å‚æ•°
        )
        
        # æˆªæ–­å¹¶æ·»åŠ åç¼€
        max_len = self.max_length - len(self._suffix_tokens)
        processed_messages = [
            ele[:max_len] + self._suffix_tokens 
            for ele in tokenized_messages
        ]
        
        # è½¬æ¢ä¸º TokensPrompt æ ¼å¼
        prompts = [
            TokensPrompt(prompt_token_ids=tokens) 
            for tokens in processed_messages
        ]
        
        return prompts
    
    def _compute_scores_batch(self, pairs: List[tuple]) -> List[float]:
        """
        ä½¿ç”¨ vLLM æ‰¹é‡è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            pairs: (query, doc) å…ƒç»„åˆ—è¡¨
            
        Returns:
            åˆ†æ•°åˆ—è¡¨
        """
        # å¤„ç†è¾“å…¥
        prompts = self._process_inputs(pairs)
        
        # ä½¿ç”¨ vLLM ç”Ÿæˆ
        outputs = self._model.generate(
            prompts, 
            self._sampling_params, 
            use_tqdm=False
        )
        
        # è®¡ç®—åˆ†æ•°
        scores = []
        for output in outputs:
            # è·å–æœ€åä¸€ä¸ª token çš„ logprobs
            final_logits = output.outputs[0].logprobs[-1]
            
            # æå– yes/no çš„ logprob
            if self._token_true_id not in final_logits:
                true_logit = -10  # æå°å€¼
            else:
                true_logit = final_logits[self._token_true_id].logprob
            
            if self._token_false_id not in final_logits:
                false_logit = -10
            else:
                false_logit = final_logits[self._token_false_id].logprob
            
            # è®¡ç®—å½’ä¸€åŒ–åˆ†æ•°
            true_score = math.exp(true_logit)
            false_score = math.exp(false_logit)
            score = true_score / (true_score + false_score)
            
            scores.append(score)
        
        return scores
    
    def _compute_scores(self, pairs: List[tuple]) -> List[float]:
        """
        åˆ†æ‰¹è®¡ç®—æ‰€æœ‰ pairs çš„ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            pairs: (query, doc) å…ƒç»„åˆ—è¡¨
            
        Returns:
            æ‰€æœ‰ pairs çš„åˆ†æ•°åˆ—è¡¨
        """
        all_scores = []
        total_pairs = len(pairs)
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total_pairs, self.batch_size):
            batch_pairs = pairs[i:i + self.batch_size]
            batch_scores = self._compute_scores_batch(batch_pairs)
            all_scores.extend(batch_scores)
            
            # æ‰“å°è¿›åº¦
            if total_pairs > self.batch_size:
                processed = min(i + self.batch_size, total_pairs)
                log(f"Reranking progress: {processed}/{total_pairs} pairs processed")
        
        return all_scores
    
    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -> List[NodeWithScore]:
        """
        é‡æ’åºèŠ‚ç‚¹(å¿…é¡»å®ç°çš„æŠ½è±¡æ–¹æ³•)
        
        Args:
            nodes: å¾…é‡æ’åºçš„èŠ‚ç‚¹åˆ—è¡¨
            query_bundle: æŸ¥è¯¢ä¿¡æ¯
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        if query_bundle is None:
            raise ValueError("Missing query bundle in extra info.")
        
        if len(nodes) == 0:
            return []
        
        # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
        query_str = query_bundle.query_str
        pairs = [
            (
                query_str,
                node.node.get_content(metadata_mode=MetadataMode.EMBED),
            )
            for node in nodes
        ]
        
        # ä½¿ç”¨ callback manager è®°å½•äº‹ä»¶
        with self.callback_manager.event(
            CBEventType.RERANKING,
            payload={
                EventPayload.NODES: nodes,
                EventPayload.MODEL_NAME: self.model,
                EventPayload.QUERY_STR: query_str,
                EventPayload.TOP_K: self.top_n,
            },
        ) as event:
            # è®¡ç®—åˆ†æ•°
            scores = self._compute_scores(pairs)
            
            assert len(scores) == len(nodes), \
                f"Score count mismatch: got {len(scores)} scores for {len(nodes)} nodes"
            
            # æ›´æ–°èŠ‚ç‚¹åˆ†æ•°
            for node, score in zip(nodes, scores):
                if self.keep_retrieval_score:
                    node.node.metadata["retrieval_score"] = node.score
                node.score = float(score)
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å› top_n
            new_nodes = sorted(
                nodes, 
                key=lambda x: -x.score if x.score else 0
            )[: self.top_n]
            
            # è®°å½•ç»“æœ
            event.on_end(payload={EventPayload.NODES: new_nodes})
        
        return new_nodes
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        try:
            # é”€æ¯ vLLM çš„åˆ†å¸ƒå¼ç¯å¢ƒ
            destroy_model_parallel()
            
            # æ¸…ç† GPU ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            log("âœ“ Qwen3Reranker_vllm resources cleaned up")
        except Exception as e:
            log(f"âš  Error during cleanup: {e}")




# ==================== 1. èŠ‚ç‚¹åˆ†å‰²å™¨ ====================
class NodeSplitter:
    """å°†é•¿èŠ‚ç‚¹åˆ†å‰²æˆå¤šä¸ªå­èŠ‚ç‚¹,ä¿æŒçˆ¶å­å…³ç³»"""
    
    def __init__(self, chunk_size: int = 512, overlap_ratio: float = 0.1):
        """
        Args:
            chunk_size: å­èŠ‚ç‚¹çš„ç›®æ ‡é•¿åº¦
            overlap_ratio: é‡å æ¯”ä¾‹ (0.1 è¡¨ç¤º 10%)
        """
        self.chunk_size = chunk_size
        self.overlap_size = int(chunk_size * overlap_ratio)
            
    def split_node(self, node: NodeWithScore, parent_id: str = None) -> List[NodeWithScore]:
        """
        å°†å•ä¸ªèŠ‚ç‚¹åˆ†å‰²æˆå¤šä¸ªå­èŠ‚ç‚¹
        
        Args:
            node: åŸå§‹èŠ‚ç‚¹
            parent_id: çˆ¶èŠ‚ç‚¹ID (å¦‚æœä¸ºNone,ä½¿ç”¨node.node.node_id)
            
        Returns:
            å­èŠ‚ç‚¹åˆ—è¡¨,æ¯ä¸ªå­èŠ‚ç‚¹éƒ½ä¿ç•™çˆ¶èŠ‚ç‚¹å¼•ç”¨
        """
        text = node.node.text
        text_length = len(text)
        
        # ğŸ”¥ å®šä¹‰è¦æ’é™¤çš„å…ƒæ•°æ®é”®(ä¸ä¼ ç»™LLM)
        excluded_llm_keys = [
            'category_depth', 'languages', 'filetype', 'last_modified',
            'parent_node_id', 'chunk_index', 'is_child_node', 
            'parent_text_length', 'chunk_start', 'chunk_end',
            'matched_children'  # ğŸ”¥ æ–°å¢:æ’é™¤å­èŠ‚ç‚¹åŒ¹é…ä¿¡æ¯
        ]
        
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºchunk_size,ç›´æ¥è¿”å›åŸèŠ‚ç‚¹
        if text_length <= self.chunk_size:
            # æ·»åŠ çˆ¶èŠ‚ç‚¹IDåˆ°metadata
            node.node.metadata['parent_node_id'] = parent_id or node.node.node_id
            node.node.metadata['is_child_node'] = False
            # ğŸ”¥ é…ç½®æ’é™¤çš„å…ƒæ•°æ®
            node.node.excluded_llm_metadata_keys = excluded_llm_keys
            return [node]
        
        parent_node_id = parent_id or node.node.node_id
        child_nodes = []
        start = 0
        chunk_index = 0
        
        while start < text_length:
            end = min(start + self.chunk_size, text_length)
            chunk_text = text[start:end]
            
            # åˆ›å»ºå­èŠ‚ç‚¹
            child_node = TextNode(
                text=chunk_text,
                metadata={
                    **node.node.metadata,  # ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„metadata
                    'parent_node_id': parent_node_id,
                    'chunk_index': chunk_index,
                    'is_child_node': True,
                    'parent_text_length': text_length,
                    'chunk_start': start,
                    'chunk_end': end
                },
                excluded_embed_metadata_keys=node.node.excluded_embed_metadata_keys,
                # ğŸ”¥ é…ç½®æ’é™¤çš„å…ƒæ•°æ®é”®
                excluded_llm_metadata_keys=excluded_llm_keys,
            )
            
            # ä¿æŒåŸå§‹è¯„åˆ†
            child_node_with_score = NodeWithScore(
                node=child_node,
                score=node.score
            )
            
            child_nodes.append(child_node_with_score)
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·ç‚¹ (å¸¦é‡å )
            start += (self.chunk_size - self.overlap_size)
            chunk_index += 1
        
        return child_nodes
    
    def split_nodes(self, nodes: List[NodeWithScore]) -> tuple[List[NodeWithScore], Dict[str, NodeWithScore]]:
        """
        æ‰¹é‡åˆ†å‰²èŠ‚ç‚¹
        
        Returns:
            (å­èŠ‚ç‚¹åˆ—è¡¨, çˆ¶èŠ‚ç‚¹æ˜ å°„å­—å…¸)
        """
        all_child_nodes = []
        parent_node_map = {}  # parent_node_id -> åŸå§‹çˆ¶èŠ‚ç‚¹
        
        for node in nodes:
            parent_id = node.node.node_id
            parent_node_map[parent_id] = node  # ä¿å­˜åŸå§‹çˆ¶èŠ‚ç‚¹
            
            child_nodes = self.split_node(node, parent_id)
            all_child_nodes.extend(child_nodes)
        
        return all_child_nodes, parent_node_map


# ==================== 2. å­èŠ‚ç‚¹åˆ°çˆ¶èŠ‚ç‚¹çš„åå¤„ç†å™¨ ====================
class ChildToParentPostprocessor(BaseNodePostprocessor):
    """
    å°†rerankåçš„å­èŠ‚ç‚¹è¿˜åŸä¸ºçˆ¶èŠ‚ç‚¹
    ç­–ç•¥: å¦‚æœå¤šä¸ªå­èŠ‚ç‚¹æ¥è‡ªåŒä¸€çˆ¶èŠ‚ç‚¹,å–æœ€é«˜åˆ†çš„å­èŠ‚ç‚¹åˆ†æ•°ä½œä¸ºçˆ¶èŠ‚ç‚¹åˆ†æ•°
    """
    
    # ä½¿ç”¨ Pydantic çš„æ–¹å¼å£°æ˜å­—æ®µ
    parent_node_map: Dict[str, Any] = {}
    keep_top_k: int = 5
    
    def __init__(self, parent_node_map: Dict[str, NodeWithScore], keep_top_k: int = 5, **kwargs):
        """
        Args:
            parent_node_map: çˆ¶èŠ‚ç‚¹IDåˆ°çˆ¶èŠ‚ç‚¹çš„æ˜ å°„
            keep_top_k: æœ€ç»ˆä¿ç•™çš„çˆ¶èŠ‚ç‚¹æ•°é‡
        """
        # ä½¿ç”¨ Pydantic çš„åˆå§‹åŒ–æ–¹å¼
        super().__init__(
            parent_node_map=parent_node_map,
            keep_top_k=keep_top_k,
            **kwargs
        )
    
    def _postprocess_nodes(
        self, 
        nodes: List[NodeWithScore], 
        query_bundle: Optional[QueryBundle] = None
    ) -> List[NodeWithScore]:
        """
        å°†å­èŠ‚ç‚¹è¿˜åŸä¸ºçˆ¶èŠ‚ç‚¹
        
        è¯„åˆ†æ’åºé€»è¾‘:
        1. æŒ‰çˆ¶èŠ‚ç‚¹åˆ†ç»„æ‰€æœ‰å­èŠ‚ç‚¹
        2. æ¯ä¸ªçˆ¶èŠ‚ç‚¹çš„å¾—åˆ† = å…¶æ‰€æœ‰å­èŠ‚ç‚¹çš„æœ€é«˜åˆ†
        3. æŒ‰çˆ¶èŠ‚ç‚¹å¾—åˆ†é™åºæ’åº
        4. è¿”å›å‰ keep_top_k ä¸ªçˆ¶èŠ‚ç‚¹
        """
        # æŒ‰çˆ¶èŠ‚ç‚¹IDåˆ†ç»„,è®°å½•æ¯ä¸ªçˆ¶èŠ‚ç‚¹çš„æœ€é«˜åˆ†æ•°
        parent_scores: Dict[str, float] = {}
        parent_child_nodes: Dict[str, List[NodeWithScore]] = {}
        
        for node in nodes:
            parent_id = node.node.metadata.get('parent_node_id')
            
            if not parent_id:
                # å¦‚æœæ²¡æœ‰çˆ¶èŠ‚ç‚¹ID,è¯´æ˜æ˜¯åŸå§‹èŠ‚ç‚¹,ç›´æ¥ä¿ç•™
                parent_scores[node.node.node_id] = node.score
                parent_child_nodes[node.node.node_id] = [node]
                continue
            
            # è®°å½•æœ€é«˜åˆ†æ•°
            if parent_id not in parent_scores:
                parent_scores[parent_id] = node.score
                parent_child_nodes[parent_id] = [node]
            else:
                # å–æœ€é«˜åˆ†
                parent_scores[parent_id] = max(parent_scores[parent_id], node.score)
                parent_child_nodes[parent_id].append(node)
        
        # ğŸ”¥ å®šä¹‰è¦æ’é™¤çš„å…ƒæ•°æ®é”®(ä¸ä¼ ç»™LLM)
        excluded_llm_keys = [
            'category_depth', 'languages', 'filetype', 'last_modified',
            'parent_node_id', 'chunk_index', 'is_child_node', 
            'parent_text_length', 'chunk_start', 'chunk_end',
            'matched_children'  # æ’é™¤å­èŠ‚ç‚¹åŒ¹é…ä¿¡æ¯
        ]
        
        # æ„å»ºçˆ¶èŠ‚ç‚¹åˆ—è¡¨
        parent_nodes = []
        for parent_id, score in parent_scores.items():
            if parent_id in self.parent_node_map:
                # ä½¿ç”¨ä¿å­˜çš„åŸå§‹çˆ¶èŠ‚ç‚¹
                parent_node = copy.deepcopy(self.parent_node_map[parent_id])
                parent_node.score = score  # ğŸ”¥ è®¾ç½®ä¸ºæ‰€æœ‰å­èŠ‚ç‚¹çš„æœ€é«˜åˆ†
                
                # ğŸ”¥ é…ç½®æ’é™¤çš„å…ƒæ•°æ®é”®
                parent_node.node.excluded_llm_metadata_keys = excluded_llm_keys
                
                # # ğŸ”¥ å¯é€‰: è®°å½•åŒ¹é…çš„å­èŠ‚ç‚¹ä¿¡æ¯(ç”¨äºè°ƒè¯•,ä½†ä¸ä¼šä¼ ç»™LLM)
                # child_info = [
                #     {
                #         'chunk_index': n.node.metadata.get('chunk_index'),
                #         'score': n.score,
                #         'text_preview': n.node.text[:50] + '...'
                #     }
                #     for n in sorted(parent_child_nodes[parent_id], key=lambda x: x.score, reverse=True)
                # ]
                # parent_node.node.metadata['matched_children'] = child_info
                
                parent_nodes.append(parent_node)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹,ä½¿ç”¨ç¬¬ä¸€ä¸ªå­èŠ‚ç‚¹(ä¸åº”è¯¥å‘ç”Ÿ)
                log(f"è­¦å‘Š: æ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ {parent_id}, ä½¿ç”¨å­èŠ‚ç‚¹ä»£æ›¿")
                fallback_node = parent_child_nodes[parent_id][0]
                fallback_node.node.excluded_llm_metadata_keys = excluded_llm_keys
                parent_nodes.append(fallback_node)
        
        # ğŸ”¥ æŒ‰åˆ†æ•°é™åºæ’åºå¹¶è¿”å›top_k
        parent_nodes.sort(key=lambda x: x.score, reverse=True)
        return parent_nodes[:self.keep_top_k]
    
    class Config:
        arbitrary_types_allowed = True  # å…è®¸ä»»æ„ç±»å‹


# ==================== 3. è‡ªå®šä¹‰æ£€ç´¢å™¨åŒ…è£…å™¨ ====================
class SplitNodeRetriever(BaseRetriever):
    """
    åŒ…è£…åŸå§‹æ£€ç´¢å™¨,è‡ªåŠ¨å¤„ç†èŠ‚ç‚¹åˆ†å‰²
    """
    
    def __init__(
        self, 
        base_retriever: BaseRetriever,
        chunk_size: int = 512,
        overlap_ratio: float = 0.1
    ):
        """
        Args:
            base_retriever: åŸå§‹æ··åˆæ£€ç´¢å™¨
            chunk_size: å­èŠ‚ç‚¹å¤§å°
            overlap_ratio: é‡å æ¯”ä¾‹
        """
        super().__init__()
        self.base_retriever = base_retriever
        self.node_splitter = NodeSplitter(chunk_size, overlap_ratio)
        self.parent_node_map = {}
    
    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """
        æ£€ç´¢å¹¶åˆ†å‰²èŠ‚ç‚¹
        """
        # 1. ä½¿ç”¨åŸå§‹æ£€ç´¢å™¨æ£€ç´¢
        nodes = self.base_retriever.retrieve(query_bundle)
        
        # 2. åˆ†å‰²èŠ‚ç‚¹
        child_nodes, self.parent_node_map = self.node_splitter.split_nodes(nodes)
        
        log(f"åŸå§‹èŠ‚ç‚¹æ•°: {len(nodes)}, åˆ†å‰²åå­èŠ‚ç‚¹æ•°: {len(child_nodes)}")
        
        return child_nodes
    
    def get_parent_node_map(self) -> Dict[str, NodeWithScore]:
        """è·å–çˆ¶èŠ‚ç‚¹æ˜ å°„,ä¾›åå¤„ç†å™¨ä½¿ç”¨"""
        return self.parent_node_map


def create_parent_postprocessor(retriever: SplitNodeRetriever, keep_top_k: int = 5):
    """åŠ¨æ€åˆ›å»ºçˆ¶èŠ‚ç‚¹åå¤„ç†å™¨"""
    return ChildToParentPostprocessor(
        parent_node_map=retriever.get_parent_node_map(),
        keep_top_k=keep_top_k
    )

class DynamicQueryEngine:
    """æ”¯æŒåŠ¨æ€åå¤„ç†å™¨çš„æŸ¥è¯¢å¼•æ“"""
    
    def __init__(
        self, 
        retriever, 
        response_synthesizer, 
        reranker, 
        keep_top_k=5,
        use_parent_nodes=True,  # ğŸ”¥ æ–°å¢å¼€å…³
        reorder=None
    ):
        self.retriever = retriever
        self.response_synthesizer = response_synthesizer
        self.reranker = reranker
        self.keep_top_k = keep_top_k
        self.use_parent_nodes = use_parent_nodes  # ğŸ”¥ ä¿å­˜å¼€å…³
        self.reorder = reorder

    def longcontext_postprocess_nodes(
        self,
        nodes: List[NodeWithScore]
    ) -> List[NodeWithScore]:
        """Postprocess nodes."""
        reordered_nodes: List[NodeWithScore] = []
        ordered_nodes: List[NodeWithScore] = sorted(
            nodes, key=lambda x: x.score if x.score is not None else 0
        )
        for i, node in enumerate(ordered_nodes):
            if i % 2 == 0:
                reordered_nodes.insert(0, node)
            else:
                reordered_nodes.append(node)
        return reordered_nodes
    
    def query(self, query_str: str):
        from llama_index.core.schema import QueryBundle
        
        # è®°å½•æ€»å¼€å§‹æ—¶é—´
        total_start = time.time()
        timing_stats: Dict[str, float] = {}
        
        # 1. æ£€ç´¢ (è‡ªåŠ¨åˆ†å‰²èŠ‚ç‚¹)
        retrieval_start = time.time()
        query_bundle = QueryBundle(query_str=query_str)
        nodes = self.retriever.retrieve(query_bundle)
        timing_stats['æ£€ç´¢'] = time.time() - retrieval_start
        
        # 2. Rerankå­èŠ‚ç‚¹
        rerank_start = time.time()
        reranked_nodes = self.reranker.postprocess_nodes(nodes, query_bundle)
        timing_stats['Rerank'] = time.time() - rerank_start
        
        # 3. æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦è¿˜åŸçˆ¶èŠ‚ç‚¹
        parent_start = time.time()
        if self.use_parent_nodes:
            parent_postprocessor = create_parent_postprocessor(
                self.retriever, 
                keep_top_k=self.keep_top_k
            )
            final_nodes = parent_postprocessor.postprocess_nodes(reranked_nodes, query_bundle)
            timing_stats['è¿˜åŸçˆ¶èŠ‚ç‚¹'] = time.time() - parent_start
        else:
            final_nodes = reranked_nodes[:self.keep_top_k]
            timing_stats['æˆªå–èŠ‚ç‚¹'] = time.time() - parent_start
        
        # 4. Reorder (å¦‚æœå¯ç”¨)
        if self.reorder:
            reorder_start = time.time()
            final_nodes = self.longcontext_postprocess_nodes(final_nodes)
            timing_stats['Reorder'] = time.time() - reorder_start
        
        # 5. ç”Ÿæˆå›ç­”
        synthesis_start = time.time()
        response = self.response_synthesizer.synthesize(
            query=query_str,
            nodes=final_nodes
        )
        timing_stats['ç”Ÿæˆå›ç­”'] = time.time() - synthesis_start
        
        # è®¡ç®—æ€»è€—æ—¶
        timing_stats['æ€»è€—æ—¶'] = time.time() - total_start
        log('a fking test')
        
        # ç®€æ´çš„è€—æ—¶è¾“å‡º
        log(f"æ£€ç´¢: {timing_stats['æ£€ç´¢']:.2f}s | Rerank: {timing_stats['Rerank']:.2f}s | ç”Ÿæˆ: {timing_stats['ç”Ÿæˆå›ç­”']:.2f}s | æ€»è®¡: {timing_stats['æ€»è€—æ—¶']:.2f}s")
        
        return response