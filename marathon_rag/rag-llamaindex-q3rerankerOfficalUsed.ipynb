{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24946b-b151-4429-9ac6-79491fb9f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama_index transformers unstructured pymilvus\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-extractors-entity\n",
    "!pip install llama-index-vector-stores-milvus\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-llms-dashscope\n",
    "!pip install llama-index-extractors\n",
    "!pip install pymilvus[milvus_lite]\n",
    "!pip install unstructured[docx]\n",
    "!pip install unstructured[doc]\n",
    "!pip install unstructured[txt]\n",
    "!pip install unstructured[md]\n",
    "!pip install fitz frontend tools\n",
    "!pip uninstall fitz pymupdf -y\n",
    "!pip install pymupdf\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd904dc2-b816-432f-bf16-6790cb770384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage\n",
    "    , Document, Settings, StorageContext, PromptTemplate)\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.extractors import KeywordExtractor, SummaryExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.dashscope import DashScope\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.readers.file import UnstructuredReader,PyMuPDFReader,PDFReader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "import os, re, asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b343e-a87c-4676-8a87-9f4c499b83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python pdf2md.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c75a3f-3416-499a-9ec9-edc255829118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:08:04,518 - INFO - Load pretrained SentenceTransformer: /root/autodl-tmp/Qwen3-Embedding-0.6B\n",
      "2025-10-16 14:08:05,591 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型嵌入维度: 1024\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"/root/autodl-tmp/Qwen3-Embedding-0.6B\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    cache_folder=None,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(embedding_model, trust_remote_code=True, local_files_only=True)\n",
    "dimension = config.hidden_size\n",
    "log(f\"模型嵌入维度: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77249092-c390-4c43-a08d-44631dff2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 请求已保存到: llm_requests/requests_log.txt\n",
      "你好！我是通义千问（Qwen），是阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，欢迎随时告诉我！\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core import Settings\n",
    "from typing import Any\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "class SiliconFlowLLM(CustomLLM):\n",
    "    \"\"\"硅基流动自定义 LLM\"\"\"\n",
    "    \n",
    "    model: str = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "    api_key: str = \"\"\n",
    "    api_base: str = \"https://api.siliconflow.cn/v1\"\n",
    "    max_tokens: int = 4096\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    # 新增：保存请求的开关和配置\n",
    "    save_requests: bool = True  # 默认开启，设为 False 则关闭\n",
    "    save_dir: str = \"llm_requests\"  # 保存目录\n",
    "    save_filename: str = \"requests_log.txt\"  # 文件名\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"获取 LLM 元数据\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=32768,  # 根据具体模型调整\n",
    "            num_output=self.max_tokens,\n",
    "            model_name=self.model,\n",
    "        )\n",
    "    \n",
    "    def _save_request(self, prompt: str, response_text: str = None):\n",
    "        \"\"\"内部方法：保存请求到文件\"\"\"\n",
    "        if not self.save_requests:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # 创建保存目录\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            \n",
    "            # 生成文件路径\n",
    "            filepath = os.path.join(self.save_dir, self.save_filename)\n",
    "            \n",
    "            # 准备保存内容\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            separator = \"=\" * 80\n",
    "            \n",
    "            content = f\"\\n{separator}\\n\"\n",
    "            content += f\"时间: {timestamp}\\n\"\n",
    "            content += f\"模型: {self.model}\\n\"\n",
    "            content += f\"{separator}\\n\"\n",
    "            content += f\"【请求内容】\\n{prompt}\\n\"\n",
    "            \n",
    "            if response_text:\n",
    "                content += f\"\\n【响应内容】\\n{response_text}\\n\"\n",
    "            \n",
    "            content += f\"{separator}\\n\"\n",
    "            \n",
    "            # 追加写入文件\n",
    "            with open(filepath, 'a', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "                \n",
    "            log(f\"✓ 请求已保存到: {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log(f\"✗ 保存请求失败: {str(e)}\")\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        \"\"\"完成请求\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_base}/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # 保存请求和响应\n",
    "        self._save_request(prompt, response_text)\n",
    "        \n",
    "        return CompletionResponse(\n",
    "            text=response_text\n",
    "        )\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any):\n",
    "        \"\"\"流式完成（未实现，但需要定义）\"\"\"\n",
    "        # 调用非流式方法\n",
    "        response = self.complete(prompt, **kwargs)\n",
    "        yield response\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 创建自定义 LLM 实例\n",
    "    llm = SiliconFlowLLM(\n",
    "        model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\",  # 可选其他模型\n",
    "        api_key=\"sk-ionsbeieleeekwlstqotkyrmictdzshgnbaytavcudxkixcs\",  # 替换为你的 API Key\n",
    "        api_base=\"https://api.siliconflow.cn/v1\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.3,\n",
    "        # 控制保存功能\n",
    "        save_requests=True,  # 设为 False 可关闭保存功能\n",
    "        save_dir=\"llm_requests\",  # 可自定义保存目录\n",
    "        save_filename=\"requests_log.txt\"  # 可自定义文件名\n",
    "    )\n",
    "    \n",
    "    # 2. 设置到 Settings\n",
    "    Settings.llm = llm\n",
    "    \n",
    "    # 3. 测试使用\n",
    "    response = llm.complete(\"你好，请介绍一下你自己\")\n",
    "    log(response.text)\n",
    "    \n",
    "    # 4. 如果需要临时关闭保存功能\n",
    "    # llm.save_requests = False\n",
    "    \n",
    "    # 5. 再次测试（不会保存）\n",
    "    # response = llm.complete(\"再问一个问题\")\n",
    "    # log(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5cc3ae-1e31-4c71-aa92-7924a9545e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "绝对数据库路径: /root/marathon_rag/milvus_test/milvus_lite.db\n",
      "已创建 ./milvus 目录\n"
     ]
    }
   ],
   "source": [
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "log(f\"绝对数据库路径: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    log(\"已创建 ./milvus 目录\")\n",
    "\n",
    "\n",
    "\n",
    "# milvus_vector_store = MilvusVectorStore(\n",
    "#     uri=f\"{abs_db_path}\",\n",
    "#     collection_name=\"rag_collection\",\n",
    "#     dim=1024,\n",
    "#     overwrite=True\n",
    "# )\n",
    "# storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5baa8d-7468-432a-9932-35ba29730ec0",
   "metadata": {},
   "source": [
    "### 首次运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0475e9bf-6391-498f-bc2c-241297bf1b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "绝对数据库路径: /root/marathon_rag/milvus_test/milvus_lite.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "###首次运行\n",
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "log(f\"绝对数据库路径: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    log(\"已创建 ./milvus 目录\")\n",
    "\n",
    "\n",
    "\n",
    "milvus_vector_store = MilvusVectorStore(\n",
    "    uri=f\"{abs_db_path}\",\n",
    "    collection_name=\"rag_collection\",\n",
    "    dim=1024,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567914c9-37a4-4739-bf70-da546fa297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "    # text = re.sub(r'(\\w+\\s*){3,}\\n', '', text)\n",
    "    # text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5\\s\\.,!?]', '', text)  # 去除特殊字符，保留中英文\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a22cbd-aedb-4285-906f-fde3ae0022d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_summary_async(text, max_words=30):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = await Settings.llm.acomplete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def generate_summary(text, max_words=30):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "async def add_summaries_to_nodes_async(nodes_list):\n",
    "    tasks = [generate_summary_async(node.text) for node in nodes_list]\n",
    "\n",
    "    summaries = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"生成节点摘要进度\"):\n",
    "        summary = await future\n",
    "        summaries.append(summary)\n",
    "\n",
    "    for node, summary in zip(nodes_list, summaries):\n",
    "        node.metadata[\"node_summary\"] = summary\n",
    "        \n",
    "def add_summaries_to_nodes(nodes_list):\n",
    "    for node in tqdm(nodes_list, desc=\"生成摘要\"):\n",
    "        summary = generate_summary(node.text)\n",
    "        node.metadata[\"node_summary\"] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "619e3fe0-1330-4417-be61-b13e6867ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/Qwen3-Reranker-4B\", trust_remote_code=True)\n",
    "documents_dir = \"./docs\"\n",
    "\n",
    "file_extractor = {\n",
    "    \".docx\": UnstructuredReader(),\n",
    "    \".doc\": UnstructuredReader(),\n",
    "    \".txt\": UnstructuredReader(),\n",
    "    \".md\": UnstructuredReader(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bf8d575-4093-4782-81c5-c6d2175fae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "def load_single_file(file_path, file_extractor):\n",
    "    \"\"\"加载单个文件\"\"\"\n",
    "    try:\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        if ext in file_extractor:\n",
    "            reader = file_extractor[ext]\n",
    "            log('loading:',file_path)\n",
    "            docs = reader.load_data(file_path)\n",
    "            return docs\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        log(f\"加载文件 {file_path} 失败: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_documents_parallel(documents_dir, file_extractor, max_workers=4):\n",
    "    \"\"\"并行加载文档\"\"\"\n",
    "    all_files = []\n",
    "    for ext in file_extractor.keys():\n",
    "        all_files.extend(Path(documents_dir).rglob(f\"*{ext}\"))\n",
    "    \n",
    "    documents = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(load_single_file, str(f), file_extractor): f \n",
    "                   for f in all_files}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"加载文件\"):\n",
    "            docs = future.result()\n",
    "            documents.extend(docs)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91cf755f-70db-417a-8649-90a166ae9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_long_documents(documents, max_length=100000, overlap=0):\n",
    "    \"\"\"预处理超长文档，避免 tokenizer 处理超长文本\"\"\"\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        text_length = len(doc.text)\n",
    "        # 如果文档太长，先粗切分\n",
    "        if text_length > max_length:\n",
    "            log(f\"检测到超长文档: {text_length} 字符，进行预切分\")\n",
    "            # 按固定长度切分，带重叠\n",
    "            chunks = []\n",
    "            start = 0\n",
    "            chunk_index = 0\n",
    "            \n",
    "            while start < text_length:\n",
    "                end = min(start + max_length, text_length)\n",
    "                chunk_text = doc.text[start:end]\n",
    "                \n",
    "                # 创建新的 metadata，添加切片信息\n",
    "                new_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "                new_metadata['chunk_index'] = chunk_index\n",
    "                new_metadata['total_chunks'] = (text_length + max_length - overlap - 1) // (max_length - overlap)\n",
    "                new_metadata['is_chunked'] = True\n",
    "                \n",
    "                chunks.append(Document(text=chunk_text, metadata=new_metadata))\n",
    "                \n",
    "                # 下一个起点：当前起点 + (max_length - overlap)\n",
    "                # 这样可以保证前后重叠 overlap 个字符\n",
    "                start += (max_length - overlap)\n",
    "                chunk_index += 1\n",
    "            \n",
    "            processed_docs.extend(chunks)\n",
    "            log(f\"  切分为 {len(chunks)} 个块，每块最大 {max_length} 字符，重叠 {overlap} 字符\")\n",
    "        else:\n",
    "            processed_docs.append(doc)\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fcf3cd-934a-4566-b27a-3a969b786365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/银联“云闪付”业务管理办法.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载文件:   0%|          | 0/12 [00:00<?, ?it/s]2025-10-16 14:11:23,428 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:   8%|▊         | 1/12 [00:02<00:26,  2.41s/it]2025-10-16 14:11:23,598 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  17%|█▋        | 2/12 [00:02<00:10,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/银联“云闪付”线下使用的常见问题解答.docx\n",
      "loading: data/中国银联银行卡联网联合技术规范.doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:23,711 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  33%|███▎      | 4/12 [00:02<00:03,  2.26it/s]2025-10-16 14:11:23,741 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-16 14:11:23,788 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-16 14:11:23,813 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  58%|█████▊    | 7/12 [00:02<00:01,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载文件 data/中国银联银行卡联网联合技术规范.doc 失败: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "loading: data/2025年第三季度新能源汽车销量公布，比亚迪继续领跑.txt\n",
      "loading: data/AI手机出货量预计突破4亿部，端侧大模型成为下一代智能终端竞争核心.txt\n",
      "loading: data/多地加码楼市优化政策，全力支持刚需与改善性需求，市场信心逐步修复.txt\n",
      "loading: data/我国发布《人工智能伦理治理指南》，为企业研发划清“红线”与“护栏”.txt\n",
      "loading: data/中国银联全渠道商户服务操作手册.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:24,289 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-16 14:11:24,396 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  75%|███████▌  | 9/12 [00:03<00:00,  4.20it/s]2025-10-16 14:11:24,468 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/中国银联商户服务平台用户操作手册(机构版).md\n",
      "loading: data/银联收单扣率分类与标准.md\n",
      "loading: data/银联清算业务体系.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:24,508 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  92%|█████████▏| 11/12 [00:03<00:00,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/银行云闪付一键查卡业务说明.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:24,822 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件: 100%|██████████| 12/12 [00:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件大小:11\n",
      "节点数量:82\n"
     ]
    }
   ],
   "source": [
    "# 使用方法\n",
    "documents_dir=  \"./data\"\n",
    "documents = load_documents_parallel(documents_dir, file_extractor, max_workers=1)\n",
    "\n",
    "cleaned_documents = [Document(text=clean_text(doc.text), metadata=doc.metadata) \n",
    "                     for doc in documents]\n",
    "\n",
    "# 添加这一步：最大长度100000，前后重叠1000\n",
    "# cleaned_documents = preprocess_long_documents(\n",
    "#     cleaned_documents, \n",
    "#     max_length=100000, \n",
    "#     overlap=0\n",
    "# )\n",
    "documents = cleaned_documents\n",
    "\n",
    "log(f\"文件大小:{len(documents)}\")\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100, tokenizer=qwen_tokenizer.tokenize)  \n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "log(f\"节点数量:{len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786076ab-5328-4eba-9b90-9e5669da92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48734949-0a3b-44d5-92b2-6b4d45100eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_summaries_to_json(nodes_list, file_path=\"nodes_summaries_temp.json\"):\n",
    "#     summaries_dict = {}\n",
    "#     for idx, node in enumerate(nodes_list):\n",
    "#         summaries_dict[str(idx)] = node.metadata.get(\"node_summary\", \"\")  # 获取摘要，若无则为空\n",
    "    \n",
    "#     # 保存到 JSON\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "#     log(f\"节点摘要已保存到 {file_path}\")\n",
    "\n",
    "# def load_summaries_to_nodes(nodes_list, file_path=\"nodes_summaries.json\"):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         summaries_dict = json.load(f)\n",
    "#     sorted_keys = sorted(summaries_dict.keys(), key=int)\n",
    "\n",
    "#     for key in sorted_keys:\n",
    "#         idx = int(key)\n",
    "#         if idx < len(nodes_list):\n",
    "#             nodes_list[idx].metadata[\"node_summary\"] = summaries_dict[key]\n",
    "#         else:\n",
    "#             log(f\"警告：索引 {idx} 超出节点列表长度，跳过。\")\n",
    "    \n",
    "#     return nodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35dc24-2ae5-4cb1-8adf-14b5645bcd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da1464-2d9c-4c15-a3b0-cb2da61a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_summaries_to_json(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f633b2f-4bff-4ae3-9327-1d6a18ded997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# ============ 保存 Nodes ============\n",
    "def save_nodes(nodes, save_dir=\"./saved_nodes\"):\n",
    "    \"\"\"保存节点数据（支持pickle和json两种格式）\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 方法1: 使用 pickle 保存完整节点对象（推荐）\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "    log(f\"Nodes已保存到: {pickle_file}\")\n",
    "\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"加载节点数据\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"❌ 找不到节点文件: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    log(f\"✅ 已加载 {len(nodes)} 个节点\")\n",
    "    \n",
    "    # 验证数据\n",
    "    log(f\"📊 节点验证:\")\n",
    "    log(f\"  - 总节点数: {len(nodes)}\")\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62b589d3-18f7-4672-9653-2be5ba001a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes已保存到: saved_nodes/nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "save_nodes(nodes, save_dir=\"./saved_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775b208d-7576-4475-89ad-5cafb8a9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=Settings.embed_model,\n",
    "#     node_parser=node_parser,\n",
    "#     store_nodes_override=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "transformations = [node_parser]\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model,\n",
    "    node_parser=node_parser,\n",
    "    transformations=transformations,\n",
    "    store_nodes_override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d8e8761-356e-4966-b632-003edb975da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 索引已保存到: ./milvus_storage\n",
      "✅ 索引信息已保存到: milvus_storage/index_info.json\n",
      "📊 索引信息: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/marathon_rag/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 82, 'index_type': 'VectorStoreIndex'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ 保存 Milvus 索引 ============\n",
    "def save_milvus_index(index, persist_dir=\"./milvus_storage\"):\n",
    "    \"\"\"保存Milvus索引（持久化到本地）\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    persist_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # LlamaIndex会自动保存索引结构和docstore\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    \n",
    "    log(f\"✅ 索引已保存到: {persist_dir}\")\n",
    "    \n",
    "    # 保存索引元信息\n",
    "    index_info = {\n",
    "        'collection_name': 'rag_collection',\n",
    "        'milvus_db_path': abs_db_path,\n",
    "        'embedding_dim': dimension,\n",
    "        'total_documents': len(index.docstore.docs),\n",
    "        'index_type': 'VectorStoreIndex'\n",
    "    }\n",
    "    \n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    with open(info_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(index_info, f, ensure_ascii=False, indent=2)\n",
    "    log(f\"✅ 索引信息已保存到: {info_file}\")\n",
    "    log(f\"📊 索引信息: {index_info}\")\n",
    "\n",
    "# ============ 使用示例 ============\n",
    "# 保存索引\n",
    "save_milvus_index(index, persist_dir=\"./milvus_storage\")\n",
    "\n",
    "# 加载索引\n",
    "# index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17816c31-5fe8-4cb9-9237-f17cefd16c2c",
   "metadata": {},
   "source": [
    "### 非第一次运行 加载持久化运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103c2661-6d60-4e1c-a448-0b588aaaab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"加载节点数据\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"❌ 找不到节点文件: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    log(f\"✅ 已加载 {len(nodes)} 个节点\")\n",
    "    \n",
    "    # 验证数据\n",
    "    log(f\"📊 节点验证:\")\n",
    "    log(f\"  - 总节点数: {len(nodes)}\")\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155ebe6a-db42-4973-875e-7e484a786aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ 加载 Milvus 索引 ============\n",
    "def load_milvus_index(persist_dir=\"./storage\", milvus_db_path=None):\n",
    "    \"\"\"加载已保存的Milvus索引\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    \n",
    "    if not persist_path.exists():\n",
    "        raise FileNotFoundError(f\"❌ 找不到索引目录: {persist_dir}\")\n",
    "    \n",
    "    # 读取索引信息\n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    if info_file.exists():\n",
    "        with open(info_file, 'r', encoding='utf-8') as f:\n",
    "            index_info = json.load(f)\n",
    "        log(f\"📊 索引信息: {index_info}\")\n",
    "        milvus_db_path = milvus_db_path or index_info.get('milvus_db_path')\n",
    "    \n",
    "    # 重建 Milvus vector store\n",
    "    milvus_vector_store = MilvusVectorStore(\n",
    "        uri=milvus_db_path,\n",
    "        collection_name=\"rag_collection\",\n",
    "        dim=dimension,\n",
    "        overwrite=False  # 不覆盖已有数据\n",
    "    )\n",
    "    \n",
    "    # 重建 storage context\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=milvus_vector_store,\n",
    "        persist_dir=persist_dir\n",
    "    )\n",
    "    \n",
    "    # 加载索引\n",
    "    index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        embed_model=Settings.embed_model\n",
    "    )\n",
    "    \n",
    "    log(f\"✅ 索引已加载\")\n",
    "    log(f\"  - 文档数量: {len(index.docstore.docs)}\")\n",
    "    \n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ae0603-9369-494a-a304-a44b37d76ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已加载 2493 个节点\n",
      "📊 节点验证:\n",
      "  - 总节点数: 2493\n",
      "📊 索引信息: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 2493, 'index_type': 'VectorStoreIndex'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-10-13 17:07:30,504 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/index_store.json.\n",
      "✅ 索引已加载\n",
      "  - 文档数量: 2493\n"
     ]
    }
   ],
   "source": [
    "nodes = load_nodes(save_dir=\"./saved_nodes\")\n",
    "index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n",
    "index.embed_model=Settings.embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cedb969-684f-4273-88a3-45e4c5edca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] 分数: 0.2065 | 文件: None\n",
      "内容: .............................................................................. 37 7.4 共享关系管理 ...........................................................................................................\n",
      "\n",
      "[2] 分数: 0.1932 | 文件: None\n",
      "内容: .......................................................... 19 5.1 交易查询 .................................................................................................................................\n",
      "\n",
      "[3] 分数: 0.1927 | 文件: None\n",
      "内容: ............................................................................................................ 40 10.1.3 操作页面 ............................................................................\n"
     ]
    }
   ],
   "source": [
    "# 单次快速检索\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "nodes_test = retriever.retrieve(\"腾讯游戏 三角洲行动\")\n",
    "\n",
    "\n",
    "# 打印结果\n",
    "for i, node in enumerate(nodes_test, 1):\n",
    "    log(f\"\\n[{i}] 分数: {node.score:.4f} | 文件: {node.metadata.get('file_name')}\")\n",
    "    log(f\"内容: {node.text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b2a35-74d4-496c-afed-46c7795044ea",
   "metadata": {},
   "source": [
    "## search and rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf4934f-e7ed-479a-9c08-f0d91be86b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-retrievers-bm25\n",
    "# !pip install llama-index-packs-fusion-retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d73f2f98-4f58-400c-8ba3-f7ca568dea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever,QueryFusionRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.fusion_retriever import HybridFusionRetrieverPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17faba70-2b48-4577-9cf0-682a6074d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在代码开头添加这些调试函数\n",
    "import json\n",
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "def print_retrieved_nodes(nodes: List[NodeWithScore], title=\"检索到的节点\"):\n",
    "    \"\"\"打印检索到的节点详细信息\"\"\"\n",
    "    log(f\"\\n{'='*80}\")\n",
    "    log(f\"{title} (共 {len(nodes)} 个)\")\n",
    "    log(f\"{'='*80}\")\n",
    "    \n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        log(f\"\\n[节点 {i}]\")\n",
    "        log(f\"  分数: {node.score:.4f}\")\n",
    "        log(f\"  节点ID: {node.node.node_id}\")\n",
    "        log(f\"  文件名: {node.node.metadata.get('file_name', 'N/A')}\")\n",
    "        \n",
    "        # 如果是子节点，显示父节点信息\n",
    "        if node.node.metadata.get('is_child_node'):\n",
    "            log(f\"  父节点ID: {node.node.metadata.get('parent_node_id')}\")\n",
    "            log(f\"  子节点索引: {node.node.metadata.get('chunk_index')}\")\n",
    "        \n",
    "        # 显示文本内容（前200字符）\n",
    "        log(f\"  内容预览: {node.node.text[100]}\")\n",
    "        log(len(node.node.text))\n",
    "        \n",
    "        # 显示完整metadata\n",
    "        log(f\"  Metadata: {json.dumps(node.node.metadata, ensure_ascii=False, indent=4)}\")\n",
    "    \n",
    "    log(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "def print_prompt_to_llm(query: str, context: str, template_name=\"\"):\n",
    "    \"\"\"打印发送给LLM的完整prompt\"\"\"\n",
    "    log(f\"\\n{'='*80}\")\n",
    "    log(f\"发送给LLM的Prompt {template_name}\")\n",
    "    log(f\"{'='*80}\")\n",
    "    log(f\"\\n【用户查询】\\n{query}\")\n",
    "    log(f\"\\n【上下文信息】\\n{context}\")\n",
    "    log(f\"\\n{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cce8fc09-ed70-4785-b12b-04f0aeb0a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, MetadataMode\n",
    "from llama_index.core.bridge.pydantic import Field, PrivateAttr\n",
    "from llama_index.core.callbacks import CBEventType, EventPayload\n",
    "from typing import List, Optional, Any, Union\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class Qwen3Reranker(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    Qwen3-Reranker for reranking nodes based on relevance to query.\n",
    "    \n",
    "    Args:\n",
    "        model (str): Path to the Qwen3-Reranker model.\n",
    "        top_n (int): Number of nodes to return sorted by score. Defaults to 5.\n",
    "        device (str, optional): Device (like \"cuda\", \"cpu\") for computation. \n",
    "            If None, checks if a GPU can be used.\n",
    "        max_length (int): Maximum sequence length. Defaults to 8192.\n",
    "        instruction (str, optional): Custom instruction for reranking.\n",
    "        keep_retrieval_score (bool, optional): Whether to keep the retrieval score \n",
    "            in metadata. Defaults to False.\n",
    "        clear_cache_after_rerank (bool): Whether to clear GPU cache after reranking.\n",
    "            Defaults to True.\n",
    "        batch_size (int): Number of query-document pairs to process at once.\n",
    "            Defaults to 5. Lower values use less memory but may be slower.\n",
    "    \"\"\"\n",
    "    \n",
    "    model: str = Field(description=\"Path to Qwen3-Reranker model.\")\n",
    "    top_n: int = Field(default=5, description=\"Number of nodes to return sorted by score.\")\n",
    "    device: Optional[str] = Field(default=None, description=\"Device for computation.\")\n",
    "    max_length: int = Field(default=8192, description=\"Maximum sequence length.\")\n",
    "    instruction: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Custom instruction for reranking.\"\n",
    "    )\n",
    "    keep_retrieval_score: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether to keep the retrieval score in metadata.\",\n",
    "    )\n",
    "    clear_cache_after_rerank: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Whether to clear GPU cache after reranking.\",\n",
    "    )\n",
    "    batch_size: int = Field(\n",
    "        default=5,\n",
    "        description=\"Number of query-document pairs to process at once.\",\n",
    "    )\n",
    "    \n",
    "    # 私有属性\n",
    "    _tokenizer: Any = PrivateAttr()\n",
    "    _model: Any = PrivateAttr()\n",
    "    _device: str = PrivateAttr()\n",
    "    _token_false_id: int = PrivateAttr()\n",
    "    _token_true_id: int = PrivateAttr()\n",
    "    _prefix: str = PrivateAttr()\n",
    "    _suffix: str = PrivateAttr()\n",
    "    _prefix_tokens: List[int] = PrivateAttr()\n",
    "    _suffix_tokens: List[int] = PrivateAttr()\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        top_n: int = 5,\n",
    "        device: Optional[str] = None,\n",
    "        max_length: int = 8192,\n",
    "        instruction: Optional[str] = None,\n",
    "        keep_retrieval_score: bool = False,\n",
    "        clear_cache_after_rerank: bool = True,\n",
    "        batch_size: int = 5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # 先调用父类初始化，传递所有 Field 属性\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            top_n=top_n,\n",
    "            device=device,\n",
    "            max_length=max_length,\n",
    "            instruction=instruction,\n",
    "            keep_retrieval_score=keep_retrieval_score,\n",
    "            clear_cache_after_rerank=clear_cache_after_rerank,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # 验证 batch_size\n",
    "        if self.batch_size < 1:\n",
    "            raise ValueError(f\"batch_size must be >= 1, got {self.batch_size}\")\n",
    "        \n",
    "        # 设置默认 instruction\n",
    "        if self.instruction is None:\n",
    "            self.instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "        \n",
    "        # 推断设备\n",
    "        if self.device is None:\n",
    "            self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self._device = self.device\n",
    "        \n",
    "        # 加载 tokenizer\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model, \n",
    "            padding_side='left'\n",
    "        )\n",
    "        \n",
    "        # 加载模型\n",
    "        try:\n",
    "            self._model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model,\n",
    "                # torch_dtype=torch.float16,  # 量化\n",
    "                attn_implementation=\"flash_attention_2\"\n",
    "            ).to(self._device).eval()\n",
    "            log(\"✓ Using flash_attention_2\")\n",
    "        except Exception as e:\n",
    "            log(f\"⚠ Flash attention not available, using default: {e}\")\n",
    "            self._model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model,\n",
    "                torch_dtype=torch.float16,\n",
    "            ).to(self._device).eval()\n",
    "        \n",
    "        # 获取 yes/no token ids\n",
    "        self._token_false_id = self._tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self._token_true_id = self._tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "        \n",
    "        # 定义前缀和后缀\n",
    "        self._prefix = (\n",
    "            \"<|im_start|>system\\n\"\n",
    "            \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "            \"Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\"\n",
    "        )\n",
    "        self._suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self._prefix_tokens = self._tokenizer.encode(self._prefix, add_special_tokens=False)\n",
    "        self._suffix_tokens = self._tokenizer.encode(self._suffix, add_special_tokens=False)\n",
    "    \n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        \"\"\"返回类名，用于序列化\"\"\"\n",
    "        return \"Qwen3Reranker\"\n",
    "    \n",
    "    def _clear_gpu_cache(self):\n",
    "        \"\"\"清理 GPU 缓存和 KV cache\"\"\"\n",
    "        if self._device.startswith(\"cuda\"):\n",
    "            # 清理模型的 KV cache（如果存在）\n",
    "            if hasattr(self._model, 'clear_cache'):\n",
    "                self._model.clear_cache()\n",
    "            \n",
    "            # 清空 CUDA 缓存\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # 强制垃圾回收\n",
    "            gc.collect()\n",
    "            \n",
    "            # 可选：同步 CUDA 操作\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    def _format_instruction(self, query: str, doc: str) -> str:\n",
    "        \"\"\"格式化输入文本\"\"\"\n",
    "        return f\"<Instruct>: {self.instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "    \n",
    "    def _process_inputs(self, pairs: List[str]):\n",
    "        \"\"\"处理输入对\"\"\"\n",
    "        inputs = self._tokenizer(\n",
    "            pairs, \n",
    "            padding=False, \n",
    "            truncation='longest_first',\n",
    "            return_attention_mask=False, \n",
    "            max_length=self.max_length - len(self._prefix_tokens) - len(self._suffix_tokens)\n",
    "        )\n",
    "        \n",
    "        # 添加前缀和后缀\n",
    "        for i, ele in enumerate(inputs['input_ids']):\n",
    "            inputs['input_ids'][i] = self._prefix_tokens + ele + self._suffix_tokens\n",
    "        \n",
    "        # 填充\n",
    "        inputs = self._tokenizer.pad(\n",
    "            inputs, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # 移动到设备\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self._device)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _compute_scores_batch(self, pairs: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        计算一批 pairs 的相关性分数\n",
    "        \n",
    "        Args:\n",
    "            pairs: 格式化后的 query-document 对列表\n",
    "            \n",
    "        Returns:\n",
    "            分数列表\n",
    "        \"\"\"\n",
    "        inputs = self._process_inputs(pairs)\n",
    "        \n",
    "        try:\n",
    "            batch_scores = self._model(**inputs).logits[:, -1, :]\n",
    "            true_vector = batch_scores[:, self._token_true_id]\n",
    "            false_vector = batch_scores[:, self._token_false_id]\n",
    "            batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "            batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "            scores = batch_scores[:, 1].exp().tolist()\n",
    "            return scores\n",
    "        finally:\n",
    "            # 立即删除 inputs 释放显存\n",
    "            del inputs\n",
    "            if self._device.startswith(\"cuda\"):\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    def _compute_scores(self, pairs: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        分批计算所有 pairs 的相关性分数\n",
    "        \n",
    "        Args:\n",
    "            pairs: 格式化后的 query-document 对列表\n",
    "            \n",
    "        Returns:\n",
    "            所有 pairs 的分数列表\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        total_pairs = len(pairs)\n",
    "        \n",
    "        # 分批处理\n",
    "        for i in range(0, total_pairs, self.batch_size):\n",
    "            batch_pairs = pairs[i:i + self.batch_size]\n",
    "            batch_scores = self._compute_scores_batch(batch_pairs)\n",
    "            all_scores.extend(batch_scores)\n",
    "            \n",
    "            # 可选：打印进度\n",
    "            if total_pairs > self.batch_size:\n",
    "                processed = min(i + self.batch_size, total_pairs)\n",
    "                log(f\"Reranking progress: {processed}/{total_pairs} pairs processed\")\n",
    "        \n",
    "        return all_scores\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        重排序节点（必须实现的抽象方法）\n",
    "        \n",
    "        Args:\n",
    "            nodes: 待重排序的节点列表\n",
    "            query_bundle: 查询信息\n",
    "            \n",
    "        Returns:\n",
    "            重排序后的节点列表\n",
    "        \"\"\"\n",
    "        if query_bundle is None:\n",
    "            raise ValueError(\"Missing query bundle in extra info.\")\n",
    "        \n",
    "        if len(nodes) == 0:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # 准备查询-文档对\n",
    "            query_str = query_bundle.query_str\n",
    "            query_and_nodes = [\n",
    "                (\n",
    "                    query_str,\n",
    "                    node.node.get_content(metadata_mode=MetadataMode.EMBED),\n",
    "                )\n",
    "                for node in nodes\n",
    "            ]\n",
    "            \n",
    "            # 格式化输入\n",
    "            pairs = [\n",
    "                self._format_instruction(query, doc) \n",
    "                for query, doc in query_and_nodes\n",
    "            ]\n",
    "            \n",
    "            # 使用 callback manager 记录事件（可选但推荐）\n",
    "            with self.callback_manager.event(\n",
    "                CBEventType.RERANKING,\n",
    "                payload={\n",
    "                    EventPayload.NODES: nodes,\n",
    "                    EventPayload.MODEL_NAME: self.model,\n",
    "                    EventPayload.QUERY_STR: query_str,\n",
    "                    EventPayload.TOP_K: self.top_n,\n",
    "                },\n",
    "            ) as event:\n",
    "                # 分批处理并计算分数\n",
    "                scores = self._compute_scores(pairs)\n",
    "                \n",
    "                assert len(scores) == len(nodes), \\\n",
    "                    f\"Score count mismatch: got {len(scores)} scores for {len(nodes)} nodes\"\n",
    "                \n",
    "                # 更新节点分数\n",
    "                for node, score in zip(nodes, scores):\n",
    "                    if self.keep_retrieval_score:\n",
    "                        # 保留原始检索分数\n",
    "                        node.node.metadata[\"retrieval_score\"] = node.score\n",
    "                    node.score = float(score)\n",
    "                \n",
    "                # 按分数排序并返回 top_n\n",
    "                new_nodes = sorted(\n",
    "                    nodes, \n",
    "                    key=lambda x: -x.score if x.score else 0\n",
    "                )[: self.top_n]\n",
    "                \n",
    "                # 记录结果\n",
    "                event.on_end(payload={EventPayload.NODES: new_nodes})\n",
    "            \n",
    "            return new_nodes\n",
    "        \n",
    "        finally:\n",
    "            # 无论是否出错，都清理缓存\n",
    "            if self.clear_cache_after_rerank:\n",
    "                self._clear_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48b72b94-9a54-4740-9b4c-e139ea8fe587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Flash attention not available, using default: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a832843bebb4fbabbe16c140086cfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reranker = Qwen3Reranker(\n",
    "    model=\"/root/autodl-tmp/Qwen3-Reranker-4B\",\n",
    "    top_n=5,\n",
    "    device=\"cuda\",\n",
    "    max_length=8192,\n",
    "    instruction=\"根据用户的问题，判断文档是否包含相关答案或信息\",\n",
    "    keep_retrieval_score=True,\n",
    "    clear_cache_after_rerank=True,\n",
    "    batch_size=5  # 默认值，一次处理 5 对\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "933289c0-cd96-4170-b131-649aeada250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc515c0de21a4a8797f12a5943bf9a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at autodl-tmp/Qwen3-Reranker-4B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "Pad token ID: 151669\n",
      "Model config pad_token_id: 151669\n"
     ]
    }
   ],
   "source": [
    "# reranker_model_path = \"autodl-tmp/Qwen3-Reranker-4B\"\n",
    "\n",
    "# reranker = SentenceTransformerRerank(\n",
    "#     model=reranker_model_path,\n",
    "#     top_n=5,\n",
    "#     device=\"cuda\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# cross_encoder = reranker._model\n",
    "# reranker_tokenizer = cross_encoder.tokenizer\n",
    "# reranker_model = cross_encoder.model\n",
    "\n",
    "# special_tokens = {'pad_token': '[PAD]'}\n",
    "# num_added_tokens = reranker_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# reranker_model.resize_token_embeddings(len(reranker_tokenizer))\n",
    "\n",
    "# reranker_tokenizer.pad_token = '[PAD]'\n",
    "# reranker_tokenizer.pad_token_id = reranker_tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "# reranker_model.config.pad_token_id = reranker_tokenizer.pad_token_id\n",
    "\n",
    "# log(f\"Pad token: {reranker_tokenizer.pad_token}\")\n",
    "# log(f\"Pad token ID: {reranker_tokenizer.pad_token_id}\")\n",
    "# log(f\"Model config pad_token_id: {reranker_model.config.pad_token_id}\")\n",
    "\n",
    "# custom_instruction = \"根据用户的问题，判断文档是否包含相关答案或信息\"  # 中文场景\n",
    "# reranker = Qwen3Reranker(\n",
    "#     model_path=\"autodl-tmp/Qwen3-Reranker-4B\",\n",
    "#     top_n=5,\n",
    "#     device=\"cuda\",\n",
    "#     max_length=8192, #最长输入\n",
    "#     instruction=None,\n",
    "#     keep_retrieval_score=True  # 如果想保留原始检索分数\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55e55514-7ff9-40fa-b4b0-f8a310914217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:13:50,832 - WARNING - The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "2025-10-16 14:13:50,856 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "def reranker_tokenize(text):\n",
    "    rerank_tokenizer = AutoTokenizer.from_pretrained(\"autodl-tmp/Qwen3-Reranker-8B\", padding_side='left')\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    tokens = rerank_tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes, \n",
    "    similarity_top_k=25,\n",
    "    tokenizer=reranker_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56bcf4b9-baeb-4364-9fd6-cc14f7b069b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = index.as_retriever(similarity_top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a729b8c7-8842-4eb9-b29a-9870ef23cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=50,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ab6a74b-1e68-4487-9b27-99f0d7a578e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker.top_n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c25c162-cde9-413e-b26a-eea12ef1637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template_str = (\n",
    "    \"上下文信息如下：\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"基于提供的上下文，用中文直接回答查询，答案只能从上下文知识中获取，不要自己发挥。\\n\"\n",
    "    \"查询：{query_str}\\n\"\n",
    "    \"回答：\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"原始查询是：{query_str}\\n\"\n",
    "    \"我们已有回答：{existing_answer}\\n\"\n",
    "    \"基于以下新上下文，用中文精炼现有回答，问题的核心回答要放在最前边，然后是解释，确保完整性和准确性：\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"精炼后的回答：\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18a7a043-1de9-41a8-9ce2-f51ee7157700",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=text_qa_template,\n",
    "    refine_template=refine_template,\n",
    "    response_mode=\"compact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e34b779-00c1-4b24-85dc-bfa4ff947a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LongContextReorder\n",
    "longcontextreorder=LongContextReorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48b14fdb-5b11-4de9-8d21-05918c0e989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[reranker,longcontextreorder]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18c73b8c-df5b-40b9-bc8c-ca15e31f3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking progress: 5/38 pairs processed\n",
      "Reranking progress: 10/38 pairs processed\n",
      "Reranking progress: 15/38 pairs processed\n",
      "Reranking progress: 20/38 pairs processed\n",
      "Reranking progress: 25/38 pairs processed\n",
      "Reranking progress: 30/38 pairs processed\n",
      "Reranking progress: 35/38 pairs processed\n",
      "Reranking progress: 38/38 pairs processed\n",
      "✓ 请求已保存到: llm_requests/requests_log.txt\n",
      "根据《支付结算办法》的相关规定，针对该客户使用腾讯发行的单位卡购买一件价值11万元人民币的艺术品的交易，可能存在以下违反“支付结算办法”的情况：\n",
      "\n",
      "1. **单位卡用于大额商品交易**：  \n",
      "   根据《支付结算办法》第142条：“单位卡不得用于10万元以上的商品交易、劳务供应款项的结算。”  \n",
      "   该交易金额为11万元人民币，超过了10万元的限额，因此**违反了该条规定**。\n",
      "\n",
      "2. **单位卡资金来源与用途不符**：  \n",
      "   根据《支付结算办法》第137条：“单位卡帐户的资金一律从其基本存款帐户转帐存入，不得交存现金，不得将销货收入的款项存入其帐户。”  \n",
      "   该交易是通过单位卡支付，但购买艺术品属于非经营性支出，且未说明该资金是否从基本存款账户转帐而来。若该资金来源于非基本账户或为现金存入，则违反了单位卡资金来源的管理规定。\n",
      "\n",
      "3. **单位卡用于非经营性用途**：  \n",
      "   单位卡的使用应限于与单位经营相关的业务活动。购买艺术品用于办公室装饰，属于非经营性支出，不符合单位卡的合规使用范围，可能构成**滥用单位卡**，违反《支付结算办法》关于单位卡使用范围的限制。\n",
      "\n",
      "综上，该笔交易**违反了《支付结算办法》第142条关于单位卡不得用于10万元以上商品交易的规定**，并可能涉及单位卡资金来源和用途的违规问题。\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "您是一家大型商业银行的首席合规官。您的一位客户是腾讯的一位高管，由于其家庭关系，他也被列为“外国政要”。在腾讯2025年第二季度财报发布后的一周内，他通过贵行进行了以下交易：\n",
    "\n",
    "他使用腾讯发行的单位卡购买了一件价值11万元人民币的艺术品，摆放在办公室。\n",
    "\n",
    "他将8万元人民币现金存入个人账户，并注明这笔资金来自个人股息。\n",
    "\n",
    "他作为付款人签署了一张金额为600万元人民币的商业承兑汇票，付款期限为90天。该草案旨在为一家3D打印公司提供新的融资，该公司将使用腾讯的“混元3D模型”人工智能服务，该服务在最近的财报中被重点提及。\n",
    "\n",
    "您的任务：\n",
    "\n",
    "对于这第一笔交易，请分别找出任何可能违反“支付结算办法”的情况\"\"\"\n",
    "response = query_engine.query(query)\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb7e17f9-3478-45a2-bca6-3734af8d0f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 【步骤1：混合检索】\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 【步骤2：Rerank重排序】\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking progress: 5/38 pairs processed\n",
      "Reranking progress: 10/38 pairs processed\n",
      "Reranking progress: 15/38 pairs processed\n",
      "Reranking progress: 20/38 pairs processed\n",
      "Reranking progress: 25/38 pairs processed\n",
      "Reranking progress: 30/38 pairs processed\n",
      "Reranking progress: 35/38 pairs processed\n",
      "Reranking progress: 38/38 pairs processed\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "您是一家大型商业银行的首席合规官。您的一位客户是腾讯的一位高管，由于其家庭关系，他也被列为“外国政要”。在腾讯2025年第二季度财报发布后的一周内，他通过贵行进行了以下交易：\n",
    "\n",
    "他使用腾讯发行的单位卡购买了一件价值11万元人民币的艺术品，摆放在办公室。\n",
    "\n",
    "他将8万元人民币现金存入个人账户，并注明这笔资金来自个人股息。\n",
    "\n",
    "他作为付款人签署了一张金额为600万元人民币的商业承兑汇票，付款期限为90天。该草案旨在为一家3D打印公司提供新的融资，该公司将使用腾讯的“混元3D模型”人工智能服务，该服务在最近的财报中被重点提及。\n",
    "\n",
    "您的任务：\n",
    "\n",
    "仅根据提供的文件，回答以下问题。\n",
    "\n",
    "对于这第一笔交易，请分别找出任何可能违反“支付结算办法”的情况。请引用文件中的具体条款编号来支持您的发现。\"\"\"\n",
    "\n",
    "# # 方法1：直接使用retriever查看检索结果\n",
    "log(\"\\n🔍 【步骤1：混合检索】\")\n",
    "retrieved_nodes = hybrid_retriever.retrieve(query)\n",
    "# print_retrieved_nodes(retrieved_nodes, \"混合检索结果\")\n",
    "\n",
    "# # 方法2：查看rerank后的结果\n",
    "log(\"\\n🎯 【步骤2：Rerank重排序】\")\n",
    "from llama_index.core.schema import QueryBundle\n",
    "query_bundle = QueryBundle(query_str=query)\n",
    "reranked_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "# print_retrieved_nodes(reranked_nodes, \"Rerank后的节点\")\n",
    "\n",
    "# # # 方法3：查看最终发送给LLM的内容\n",
    "# log(\"\\n📝 【步骤3：生成回答】\")\n",
    "# # 手动构建context来查看\n",
    "# context_str = \"\\n\\n\".join([node.node.get_content() for node in reranked_nodes])\n",
    "# print_prompt_to_llm(query, context_str, \"(text_qa_template)\")\n",
    "\n",
    "# # 执行查询\n",
    "# response = query_engine.query(query)\n",
    "# log(f\"\\n✅ 【最终回答】\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06741b50-ba2e-4380-8e2d-4268b1703e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import List, Optional, Dict\n",
    "import copy\n",
    "\n",
    "# ==================== 1. 节点分割器 ====================\n",
    "class NodeSplitter:\n",
    "    \"\"\"将长节点分割成多个子节点,保持父子关系\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, overlap_ratio: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: 子节点的目标长度\n",
    "            overlap_ratio: 重叠比例 (0.1 表示 10%)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_size = int(chunk_size * overlap_ratio)\n",
    "        \n",
    "    def split_node(self, node: NodeWithScore, parent_id: str = None) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        将单个节点分割成多个子节点\n",
    "        \n",
    "        Args:\n",
    "            node: 原始节点\n",
    "            parent_id: 父节点ID (如果为None,使用node.node.node_id)\n",
    "            \n",
    "        Returns:\n",
    "            子节点列表,每个子节点都保留父节点引用\n",
    "        \"\"\"\n",
    "        text = node.node.text\n",
    "        text_length = len(text)\n",
    "        \n",
    "        # 如果文本长度小于chunk_size,直接返回原节点\n",
    "        if text_length <= self.chunk_size:\n",
    "            # 添加父节点ID到metadata\n",
    "            node.node.metadata['parent_node_id'] = parent_id or node.node.node_id\n",
    "            node.node.metadata['is_child_node'] = False\n",
    "            return [node]\n",
    "        \n",
    "        parent_node_id = parent_id or node.node.node_id\n",
    "        child_nodes = []\n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start < text_length:\n",
    "            end = min(start + self.chunk_size, text_length)\n",
    "            chunk_text = text[start:end]\n",
    "            \n",
    "            # 创建子节点\n",
    "            child_node = TextNode(\n",
    "                text=chunk_text,\n",
    "                metadata={\n",
    "                    **node.node.metadata,  # 继承父节点的metadata\n",
    "                    'parent_node_id': parent_node_id,\n",
    "                    'chunk_index': chunk_index,\n",
    "                    'is_child_node': True,\n",
    "                    'parent_text_length': text_length,\n",
    "                    'chunk_start': start,\n",
    "                    'chunk_end': end\n",
    "                },\n",
    "                excluded_embed_metadata_keys=node.node.excluded_embed_metadata_keys,\n",
    "                excluded_llm_metadata_keys=node.node.excluded_llm_metadata_keys,\n",
    "            )\n",
    "            \n",
    "            # 保持原始评分\n",
    "            child_node_with_score = NodeWithScore(\n",
    "                node=child_node,\n",
    "                score=node.score\n",
    "            )\n",
    "            \n",
    "            child_nodes.append(child_node_with_score)\n",
    "            \n",
    "            # 计算下一个起点 (带重叠)\n",
    "            start += (self.chunk_size - self.overlap_size)\n",
    "            chunk_index += 1\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def split_nodes(self, nodes: List[NodeWithScore]) -> tuple[List[NodeWithScore], Dict[str, NodeWithScore]]:\n",
    "        \"\"\"\n",
    "        批量分割节点\n",
    "        \n",
    "        Returns:\n",
    "            (子节点列表, 父节点映射字典)\n",
    "        \"\"\"\n",
    "        all_child_nodes = []\n",
    "        parent_node_map = {}  # parent_node_id -> 原始父节点\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.node_id\n",
    "            parent_node_map[parent_id] = node  # 保存原始父节点\n",
    "            \n",
    "            child_nodes = self.split_node(node, parent_id)\n",
    "            all_child_nodes.extend(child_nodes)\n",
    "        \n",
    "        return all_child_nodes, parent_node_map\n",
    "\n",
    "\n",
    "# ==================== 2. 子节点到父节点的后处理器 ====================\n",
    "class ChildToParentPostprocessor(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    将rerank后的子节点还原为父节点\n",
    "    策略: 如果多个子节点来自同一父节点,取最高分的子节点分数作为父节点分数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 使用 Pydantic 的方式声明字段\n",
    "    parent_node_map: Dict[str, Any] = {}\n",
    "    keep_top_k: int = 5\n",
    "    \n",
    "    def __init__(self, parent_node_map: Dict[str, NodeWithScore], keep_top_k: int = 5, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parent_node_map: 父节点ID到父节点的映射\n",
    "            keep_top_k: 最终保留的父节点数量\n",
    "        \"\"\"\n",
    "        # 使用 Pydantic 的初始化方式\n",
    "        super().__init__(\n",
    "            parent_node_map=parent_node_map,\n",
    "            keep_top_k=keep_top_k,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self, \n",
    "        nodes: List[NodeWithScore], \n",
    "        query_bundle: Optional[QueryBundle] = None\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        将子节点还原为父节点\n",
    "        \"\"\"\n",
    "        # 按父节点ID分组,记录每个父节点的最高分数\n",
    "        parent_scores: Dict[str, float] = {}\n",
    "        parent_child_nodes: Dict[str, List[NodeWithScore]] = {}\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.metadata.get('parent_node_id')\n",
    "            \n",
    "            if not parent_id:\n",
    "                # 如果没有父节点ID,说明是原始节点,直接保留\n",
    "                parent_scores[node.node.node_id] = node.score\n",
    "                parent_child_nodes[node.node.node_id] = [node]\n",
    "                continue\n",
    "            \n",
    "            # 记录最高分数\n",
    "            if parent_id not in parent_scores:\n",
    "                parent_scores[parent_id] = node.score\n",
    "                parent_child_nodes[parent_id] = [node]\n",
    "            else:\n",
    "                # 取最高分\n",
    "                parent_scores[parent_id] = max(parent_scores[parent_id], node.score)\n",
    "                parent_child_nodes[parent_id].append(node)\n",
    "        \n",
    "        # 构建父节点列表\n",
    "        parent_nodes = []\n",
    "        for parent_id, score in parent_scores.items():\n",
    "            if parent_id in self.parent_node_map:\n",
    "                # 使用保存的原始父节点\n",
    "                parent_node = copy.deepcopy(self.parent_node_map[parent_id])\n",
    "                parent_node.score = score\n",
    "                \n",
    "                # 可选: 在metadata中记录匹配的子节点信息\n",
    "                child_info = [\n",
    "                    {\n",
    "                        'chunk_index': n.node.metadata.get('chunk_index'),\n",
    "                        'score': n.score,\n",
    "                        'text_preview': n.node.text[:100]\n",
    "                    }\n",
    "                    for n in parent_child_nodes[parent_id]\n",
    "                ]\n",
    "                parent_node.node.metadata['matched_children'] = child_info\n",
    "                \n",
    "                parent_nodes.append(parent_node)\n",
    "            else:\n",
    "                # 如果找不到父节点,使用第一个子节点(不应该发生)\n",
    "                log(f\"警告: 找不到父节点 {parent_id}, 使用子节点代替\")\n",
    "                parent_nodes.append(parent_child_nodes[parent_id][0])\n",
    "        \n",
    "        # 按分数排序并返回top_k\n",
    "        parent_nodes.sort(key=lambda x: x.score, reverse=True)\n",
    "        return parent_nodes[:self.keep_top_k]\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # 允许任意类型\n",
    "\n",
    "# ==================== 3. 自定义检索器包装器 ====================\n",
    "class SplitNodeRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    包装原始检索器,自动处理节点分割\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_retriever: BaseRetriever,\n",
    "        chunk_size: int = 512,\n",
    "        overlap_ratio: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_retriever: 原始混合检索器\n",
    "            chunk_size: 子节点大小\n",
    "            overlap_ratio: 重叠比例\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_retriever = base_retriever\n",
    "        self.node_splitter = NodeSplitter(chunk_size, overlap_ratio)\n",
    "        self.parent_node_map = {}\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        检索并分割节点\n",
    "        \"\"\"\n",
    "        # 1. 使用原始检索器检索\n",
    "        nodes = self.base_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # 2. 分割节点\n",
    "        child_nodes, self.parent_node_map = self.node_splitter.split_nodes(nodes)\n",
    "        \n",
    "        log(f\"原始节点数: {len(nodes)}, 分割后子节点数: {len(child_nodes)}\")\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def get_parent_node_map(self) -> Dict[str, NodeWithScore]:\n",
    "        \"\"\"获取父节点映射,供后处理器使用\"\"\"\n",
    "        return self.parent_node_map\n",
    "\n",
    "\n",
    "def create_parent_postprocessor(retriever: SplitNodeRetriever, keep_top_k: int = 5):\n",
    "    \"\"\"动态创建父节点后处理器\"\"\"\n",
    "    return ChildToParentPostprocessor(\n",
    "        parent_node_map=retriever.get_parent_node_map(),\n",
    "        keep_top_k=keep_top_k\n",
    "    )\n",
    "\n",
    "\n",
    "class DynamicQueryEngine:\n",
    "    \"\"\"支持动态后处理器的查询引擎\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        retriever, \n",
    "        response_synthesizer, \n",
    "        reranker, \n",
    "        keep_top_k=5,\n",
    "        use_parent_nodes=True,\n",
    "        reorder=None\n",
    "    ):\n",
    "        self.retriever = retriever\n",
    "        self.response_synthesizer = response_synthesizer\n",
    "        self.reranker = reranker\n",
    "        self.keep_top_k = keep_top_k\n",
    "        self.use_parent_nodes = use_parent_nodes\n",
    "        self.reorder = reorder\n",
    "\n",
    "    def longcontext_postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore]\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Postprocess nodes.\"\"\"\n",
    "        reordered_nodes: List[NodeWithScore] = []\n",
    "        ordered_nodes: List[NodeWithScore] = sorted(\n",
    "            nodes, key=lambda x: x.score if x.score is not None else 0\n",
    "        )\n",
    "        for i, node in enumerate(ordered_nodes):\n",
    "            if i % 2 == 0:\n",
    "                reordered_nodes.insert(0, node)\n",
    "            else:\n",
    "                reordered_nodes.append(node)\n",
    "        return reordered_nodes\n",
    "    \n",
    "    def query(self, query_str: str):\n",
    "        from llama_index.core.schema import QueryBundle\n",
    "        \n",
    "        # 记录总开始时间\n",
    "        total_start = time.time()\n",
    "        timing_stats: Dict[str, float] = {}\n",
    "        \n",
    "        # 1. 检索 (自动分割节点)\n",
    "        retrieval_start = time.time()\n",
    "        query_bundle = QueryBundle(query_str=query_str)\n",
    "        nodes = self.retriever.retrieve(query_bundle)\n",
    "        timing_stats['检索'] = time.time() - retrieval_start\n",
    "        \n",
    "        # 2. Rerank子节点\n",
    "        rerank_start = time.time()\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(nodes, query_bundle)\n",
    "        timing_stats['Rerank'] = time.time() - rerank_start\n",
    "        \n",
    "        # 3. 根据开关决定是否还原父节点\n",
    "        parent_start = time.time()\n",
    "        if self.use_parent_nodes:\n",
    "            parent_postprocessor = create_parent_postprocessor(\n",
    "                self.retriever, \n",
    "                keep_top_k=self.keep_top_k\n",
    "            )\n",
    "            final_nodes = parent_postprocessor.postprocess_nodes(reranked_nodes, query_bundle)\n",
    "            timing_stats['还原父节点'] = time.time() - parent_start\n",
    "        else:\n",
    "            final_nodes = reranked_nodes[:self.keep_top_k]\n",
    "            timing_stats['截取节点'] = time.time() - parent_start\n",
    "        \n",
    "        # 4. Reorder (如果启用)\n",
    "        if self.reorder:\n",
    "            reorder_start = time.time()\n",
    "            final_nodes = self.longcontext_postprocess_nodes(final_nodes)\n",
    "            timing_stats['Reorder'] = time.time() - reorder_start\n",
    "        \n",
    "        # 5. 生成回答\n",
    "        synthesis_start = time.time()\n",
    "        response = self.response_synthesizer.synthesize(\n",
    "            query=query_str,\n",
    "            nodes=final_nodes\n",
    "        )\n",
    "        timing_stats['生成回答'] = time.time() - synthesis_start\n",
    "        \n",
    "        # 计算总耗时\n",
    "        timing_stats['总耗时'] = time.time() - total_start\n",
    "        \n",
    "        # 简洁的耗时输出\n",
    "        # log(f\"检索: {timing_stats['检索']:.2f}s | Rerank: {timing_stats['Rerank']:.2f}s | 生成: {timing_stats['生成回答']:.2f}s | 总计: {timing_stats['总耗时']:.2f}s\")\n",
    "\n",
    "            # 打印耗时统计\n",
    "        log(\"\\n\" + \"=\"*50)\n",
    "        log(\"⏱️  耗时统计:\")\n",
    "        log(\"=\"*50)\n",
    "        for step, duration in timing_stats.items():\n",
    "            if step != '总耗时':\n",
    "                percentage = (duration / timing_stats['总耗时']) * 100\n",
    "                log(f\"{step:12s}: {duration:6.3f}秒 ({percentage:5.1f}%)\")\n",
    "        log(\"-\"*50)\n",
    "        log(f\"{'总耗时':12s}: {timing_stats['总耗时']:6.3f}秒 (100.0%)\")\n",
    "        log(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236213d2-b889-4a13-89ce-8d5939dce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_retriever = SplitNodeRetriever(\n",
    "    base_retriever=hybrid_retriever,\n",
    "    chunk_size=256,      # 分割为512长度 (或256)\n",
    "    overlap_ratio=0    # 0%重叠\n",
    ")\n",
    "\n",
    "\n",
    "reranker.top_n=10\n",
    "reranker.batch_size=10\n",
    "\n",
    "dynamic_query_engine = DynamicQueryEngine(\n",
    "    retriever=split_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    reranker=reranker,\n",
    "    keep_top_k=10,\n",
    "    use_parent_nodes=False,  # 🔥 直接用top5子节点\n",
    "    reorder=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4879465-574b-4ca0-b7e6-dcd6c1a67aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "在云闪付业务中，总行网络金融事业部的主要职责是什么\"\"\"\n",
    "response = dynamic_query_engine.query(query)\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93e7cb-d71f-4b30-91b1-b8e13a457585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
