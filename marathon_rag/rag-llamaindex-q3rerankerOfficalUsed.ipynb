{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24946b-b151-4429-9ac6-79491fb9f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama_index transformers unstructured pymilvus\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-extractors-entity\n",
    "!pip install llama-index-vector-stores-milvus\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-llms-dashscope\n",
    "!pip install llama-index-extractors\n",
    "!pip install pymilvus[milvus_lite]\n",
    "!pip install unstructured[docx]\n",
    "!pip install unstructured[doc]\n",
    "!pip install unstructured[txt]\n",
    "!pip install unstructured[md]\n",
    "!pip install fitz frontend tools\n",
    "!pip uninstall fitz pymupdf -y\n",
    "!pip install pymupdf\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd904dc2-b816-432f-bf16-6790cb770384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage\n",
    "    , Document, Settings, StorageContext, PromptTemplate)\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.extractors import KeywordExtractor, SummaryExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.dashscope import DashScope\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.readers.file import UnstructuredReader,PyMuPDFReader,PDFReader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "import os, re, asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b343e-a87c-4676-8a87-9f4c499b83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python pdf2md.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c75a3f-3416-499a-9ec9-edc255829118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:08:04,518 - INFO - Load pretrained SentenceTransformer: /root/autodl-tmp/Qwen3-Embedding-0.6B\n",
      "2025-10-16 14:08:05,591 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åµŒå…¥ç»´åº¦: 1024\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"/root/autodl-tmp/Qwen3-Embedding-0.6B\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    cache_folder=None,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(embedding_model, trust_remote_code=True, local_files_only=True)\n",
    "dimension = config.hidden_size\n",
    "log(f\"æ¨¡å‹åµŒå…¥ç»´åº¦: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77249092-c390-4c43-a08d-44631dff2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ è¯·æ±‚å·²ä¿å­˜åˆ°: llm_requests/requests_log.txt\n",
      "ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core import Settings\n",
    "from typing import Any\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "class SiliconFlowLLM(CustomLLM):\n",
    "    \"\"\"ç¡…åŸºæµåŠ¨è‡ªå®šä¹‰ LLM\"\"\"\n",
    "    \n",
    "    model: str = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "    api_key: str = \"\"\n",
    "    api_base: str = \"https://api.siliconflow.cn/v1\"\n",
    "    max_tokens: int = 4096\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    # æ–°å¢ï¼šä¿å­˜è¯·æ±‚çš„å¼€å…³å’Œé…ç½®\n",
    "    save_requests: bool = True  # é»˜è®¤å¼€å¯ï¼Œè®¾ä¸º False åˆ™å…³é—­\n",
    "    save_dir: str = \"llm_requests\"  # ä¿å­˜ç›®å½•\n",
    "    save_filename: str = \"requests_log.txt\"  # æ–‡ä»¶å\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"è·å– LLM å…ƒæ•°æ®\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=32768,  # æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´\n",
    "            num_output=self.max_tokens,\n",
    "            model_name=self.model,\n",
    "        )\n",
    "    \n",
    "    def _save_request(self, prompt: str, response_text: str = None):\n",
    "        \"\"\"å†…éƒ¨æ–¹æ³•ï¼šä¿å­˜è¯·æ±‚åˆ°æ–‡ä»¶\"\"\"\n",
    "        if not self.save_requests:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # åˆ›å»ºä¿å­˜ç›®å½•\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            \n",
    "            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„\n",
    "            filepath = os.path.join(self.save_dir, self.save_filename)\n",
    "            \n",
    "            # å‡†å¤‡ä¿å­˜å†…å®¹\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            separator = \"=\" * 80\n",
    "            \n",
    "            content = f\"\\n{separator}\\n\"\n",
    "            content += f\"æ—¶é—´: {timestamp}\\n\"\n",
    "            content += f\"æ¨¡å‹: {self.model}\\n\"\n",
    "            content += f\"{separator}\\n\"\n",
    "            content += f\"ã€è¯·æ±‚å†…å®¹ã€‘\\n{prompt}\\n\"\n",
    "            \n",
    "            if response_text:\n",
    "                content += f\"\\nã€å“åº”å†…å®¹ã€‘\\n{response_text}\\n\"\n",
    "            \n",
    "            content += f\"{separator}\\n\"\n",
    "            \n",
    "            # è¿½åŠ å†™å…¥æ–‡ä»¶\n",
    "            with open(filepath, 'a', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "                \n",
    "            log(f\"âœ“ è¯·æ±‚å·²ä¿å­˜åˆ°: {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log(f\"âœ— ä¿å­˜è¯·æ±‚å¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        \"\"\"å®Œæˆè¯·æ±‚\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_base}/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # ä¿å­˜è¯·æ±‚å’Œå“åº”\n",
    "        self._save_request(prompt, response_text)\n",
    "        \n",
    "        return CompletionResponse(\n",
    "            text=response_text\n",
    "        )\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any):\n",
    "        \"\"\"æµå¼å®Œæˆï¼ˆæœªå®ç°ï¼Œä½†éœ€è¦å®šä¹‰ï¼‰\"\"\"\n",
    "        # è°ƒç”¨éæµå¼æ–¹æ³•\n",
    "        response = self.complete(prompt, **kwargs)\n",
    "        yield response\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. åˆ›å»ºè‡ªå®šä¹‰ LLM å®ä¾‹\n",
    "    llm = SiliconFlowLLM(\n",
    "        model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\",  # å¯é€‰å…¶ä»–æ¨¡å‹\n",
    "        api_key=\"sk-ionsbeieleeekwlstqotkyrmictdzshgnbaytavcudxkixcs\",  # æ›¿æ¢ä¸ºä½ çš„ API Key\n",
    "        api_base=\"https://api.siliconflow.cn/v1\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.3,\n",
    "        # æ§åˆ¶ä¿å­˜åŠŸèƒ½\n",
    "        save_requests=True,  # è®¾ä¸º False å¯å…³é—­ä¿å­˜åŠŸèƒ½\n",
    "        save_dir=\"llm_requests\",  # å¯è‡ªå®šä¹‰ä¿å­˜ç›®å½•\n",
    "        save_filename=\"requests_log.txt\"  # å¯è‡ªå®šä¹‰æ–‡ä»¶å\n",
    "    )\n",
    "    \n",
    "    # 2. è®¾ç½®åˆ° Settings\n",
    "    Settings.llm = llm\n",
    "    \n",
    "    # 3. æµ‹è¯•ä½¿ç”¨\n",
    "    response = llm.complete(\"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\")\n",
    "    log(response.text)\n",
    "    \n",
    "    # 4. å¦‚æœéœ€è¦ä¸´æ—¶å…³é—­ä¿å­˜åŠŸèƒ½\n",
    "    # llm.save_requests = False\n",
    "    \n",
    "    # 5. å†æ¬¡æµ‹è¯•ï¼ˆä¸ä¼šä¿å­˜ï¼‰\n",
    "    # response = llm.complete(\"å†é—®ä¸€ä¸ªé—®é¢˜\")\n",
    "    # log(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5cc3ae-1e31-4c71-aa92-7924a9545e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»å¯¹æ•°æ®åº“è·¯å¾„: /root/marathon_rag/milvus_test/milvus_lite.db\n",
      "å·²åˆ›å»º ./milvus ç›®å½•\n"
     ]
    }
   ],
   "source": [
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "log(f\"ç»å¯¹æ•°æ®åº“è·¯å¾„: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    log(\"å·²åˆ›å»º ./milvus ç›®å½•\")\n",
    "\n",
    "\n",
    "\n",
    "# milvus_vector_store = MilvusVectorStore(\n",
    "#     uri=f\"{abs_db_path}\",\n",
    "#     collection_name=\"rag_collection\",\n",
    "#     dim=1024,\n",
    "#     overwrite=True\n",
    "# )\n",
    "# storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5baa8d-7468-432a-9932-35ba29730ec0",
   "metadata": {},
   "source": [
    "### é¦–æ¬¡è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0475e9bf-6391-498f-bc2c-241297bf1b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»å¯¹æ•°æ®åº“è·¯å¾„: /root/marathon_rag/milvus_test/milvus_lite.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "###é¦–æ¬¡è¿è¡Œ\n",
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "log(f\"ç»å¯¹æ•°æ®åº“è·¯å¾„: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    log(\"å·²åˆ›å»º ./milvus ç›®å½•\")\n",
    "\n",
    "\n",
    "\n",
    "milvus_vector_store = MilvusVectorStore(\n",
    "    uri=f\"{abs_db_path}\",\n",
    "    collection_name=\"rag_collection\",\n",
    "    dim=1024,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567914c9-37a4-4739-bf70-da546fa297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "    # text = re.sub(r'(\\w+\\s*){3,}\\n', '', text)\n",
    "    # text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5\\s\\.,!?]', '', text)  # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a22cbd-aedb-4285-906f-fde3ae0022d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_summary_async(text, max_words=30):\n",
    "    prompt = f\"æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼Œä¸è¶…è¿‡{max_words}å­—ï¼Œç›´æ¥å›å¤ç»“æœï¼š{text}\"\n",
    "    response = await Settings.llm.acomplete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def generate_summary(text, max_words=30):\n",
    "    prompt = f\"æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼Œä¸è¶…è¿‡{max_words}å­—ï¼Œç›´æ¥å›å¤ç»“æœï¼š{text}\"\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "async def add_summaries_to_nodes_async(nodes_list):\n",
    "    tasks = [generate_summary_async(node.text) for node in nodes_list]\n",
    "\n",
    "    summaries = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦è¿›åº¦\"):\n",
    "        summary = await future\n",
    "        summaries.append(summary)\n",
    "\n",
    "    for node, summary in zip(nodes_list, summaries):\n",
    "        node.metadata[\"node_summary\"] = summary\n",
    "        \n",
    "def add_summaries_to_nodes(nodes_list):\n",
    "    for node in tqdm(nodes_list, desc=\"ç”Ÿæˆæ‘˜è¦\"):\n",
    "        summary = generate_summary(node.text)\n",
    "        node.metadata[\"node_summary\"] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "619e3fe0-1330-4417-be61-b13e6867ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/Qwen3-Reranker-4B\", trust_remote_code=True)\n",
    "documents_dir = \"./docs\"\n",
    "\n",
    "file_extractor = {\n",
    "    \".docx\": UnstructuredReader(),\n",
    "    \".doc\": UnstructuredReader(),\n",
    "    \".txt\": UnstructuredReader(),\n",
    "    \".md\": UnstructuredReader(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bf8d575-4093-4782-81c5-c6d2175fae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "def load_single_file(file_path, file_extractor):\n",
    "    \"\"\"åŠ è½½å•ä¸ªæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        if ext in file_extractor:\n",
    "            reader = file_extractor[ext]\n",
    "            log('loading:',file_path)\n",
    "            docs = reader.load_data(file_path)\n",
    "            return docs\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        log(f\"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_documents_parallel(documents_dir, file_extractor, max_workers=4):\n",
    "    \"\"\"å¹¶è¡ŒåŠ è½½æ–‡æ¡£\"\"\"\n",
    "    all_files = []\n",
    "    for ext in file_extractor.keys():\n",
    "        all_files.extend(Path(documents_dir).rglob(f\"*{ext}\"))\n",
    "    \n",
    "    documents = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(load_single_file, str(f), file_extractor): f \n",
    "                   for f in all_files}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"åŠ è½½æ–‡ä»¶\"):\n",
    "            docs = future.result()\n",
    "            documents.extend(docs)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91cf755f-70db-417a-8649-90a166ae9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_long_documents(documents, max_length=100000, overlap=0):\n",
    "    \"\"\"é¢„å¤„ç†è¶…é•¿æ–‡æ¡£ï¼Œé¿å… tokenizer å¤„ç†è¶…é•¿æ–‡æœ¬\"\"\"\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        text_length = len(doc.text)\n",
    "        # å¦‚æœæ–‡æ¡£å¤ªé•¿ï¼Œå…ˆç²—åˆ‡åˆ†\n",
    "        if text_length > max_length:\n",
    "            log(f\"æ£€æµ‹åˆ°è¶…é•¿æ–‡æ¡£: {text_length} å­—ç¬¦ï¼Œè¿›è¡Œé¢„åˆ‡åˆ†\")\n",
    "            # æŒ‰å›ºå®šé•¿åº¦åˆ‡åˆ†ï¼Œå¸¦é‡å \n",
    "            chunks = []\n",
    "            start = 0\n",
    "            chunk_index = 0\n",
    "            \n",
    "            while start < text_length:\n",
    "                end = min(start + max_length, text_length)\n",
    "                chunk_text = doc.text[start:end]\n",
    "                \n",
    "                # åˆ›å»ºæ–°çš„ metadataï¼Œæ·»åŠ åˆ‡ç‰‡ä¿¡æ¯\n",
    "                new_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "                new_metadata['chunk_index'] = chunk_index\n",
    "                new_metadata['total_chunks'] = (text_length + max_length - overlap - 1) // (max_length - overlap)\n",
    "                new_metadata['is_chunked'] = True\n",
    "                \n",
    "                chunks.append(Document(text=chunk_text, metadata=new_metadata))\n",
    "                \n",
    "                # ä¸‹ä¸€ä¸ªèµ·ç‚¹ï¼šå½“å‰èµ·ç‚¹ + (max_length - overlap)\n",
    "                # è¿™æ ·å¯ä»¥ä¿è¯å‰åé‡å  overlap ä¸ªå­—ç¬¦\n",
    "                start += (max_length - overlap)\n",
    "                chunk_index += 1\n",
    "            \n",
    "            processed_docs.extend(chunks)\n",
    "            log(f\"  åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—ï¼Œæ¯å—æœ€å¤§ {max_length} å­—ç¬¦ï¼Œé‡å  {overlap} å­—ç¬¦\")\n",
    "        else:\n",
    "            processed_docs.append(doc)\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fcf3cd-934a-4566-b27a-3a969b786365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/é“¶è”â€œäº‘é—ªä»˜â€ä¸šåŠ¡ç®¡ç†åŠæ³•.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ–‡ä»¶:   0%|          | 0/12 [00:00<?, ?it/s]2025-10-16 14:11:23,428 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:   8%|â–Š         | 1/12 [00:02<00:26,  2.41s/it]2025-10-16 14:11:23,598 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  17%|â–ˆâ–‹        | 2/12 [00:02<00:10,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/é“¶è”â€œäº‘é—ªä»˜â€çº¿ä¸‹ä½¿ç”¨çš„å¸¸è§é—®é¢˜è§£ç­”.docx\n",
      "loading: data/ä¸­å›½é“¶è”é“¶è¡Œå¡è”ç½‘è”åˆæŠ€æœ¯è§„èŒƒ.doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:23,711 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  33%|â–ˆâ–ˆâ–ˆâ–      | 4/12 [00:02<00:03,  2.26it/s]2025-10-16 14:11:23,741 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-16 14:11:23,788 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-16 14:11:23,813 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:02<00:01,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ–‡ä»¶ data/ä¸­å›½é“¶è”é“¶è¡Œå¡è”ç½‘è”åˆæŠ€æœ¯è§„èŒƒ.doc å¤±è´¥: soffice command was not found. Please install libreoffice\n",
      "on your system and try again.\n",
      "\n",
      "- Install instructions: https://www.libreoffice.org/get-help/install-howto/\n",
      "- Mac: https://formulae.brew.sh/cask/libreoffice\n",
      "- Debian: https://wiki.debian.org/LibreOffice\n",
      "loading: data/2025å¹´ç¬¬ä¸‰å­£åº¦æ–°èƒ½æºæ±½è½¦é”€é‡å…¬å¸ƒï¼Œæ¯”äºšè¿ªç»§ç»­é¢†è·‘.txt\n",
      "loading: data/AIæ‰‹æœºå‡ºè´§é‡é¢„è®¡çªç ´4äº¿éƒ¨ï¼Œç«¯ä¾§å¤§æ¨¡å‹æˆä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ç»ˆç«¯ç«äº‰æ ¸å¿ƒ.txt\n",
      "loading: data/å¤šåœ°åŠ ç æ¥¼å¸‚ä¼˜åŒ–æ”¿ç­–ï¼Œå…¨åŠ›æ”¯æŒåˆšéœ€ä¸æ”¹å–„æ€§éœ€æ±‚ï¼Œå¸‚åœºä¿¡å¿ƒé€æ­¥ä¿®å¤.txt\n",
      "loading: data/æˆ‘å›½å‘å¸ƒã€Šäººå·¥æ™ºèƒ½ä¼¦ç†æ²»ç†æŒ‡å—ã€‹ï¼Œä¸ºä¼ä¸šç ”å‘åˆ’æ¸…â€œçº¢çº¿â€ä¸â€œæŠ¤æ â€.txt\n",
      "loading: data/ä¸­å›½é“¶è”å…¨æ¸ é“å•†æˆ·æœåŠ¡æ“ä½œæ‰‹å†Œ.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:24,289 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-16 14:11:24,396 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:03<00:00,  4.20it/s]2025-10-16 14:11:24,468 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/ä¸­å›½é“¶è”å•†æˆ·æœåŠ¡å¹³å°ç”¨æˆ·æ“ä½œæ‰‹å†Œ(æœºæ„ç‰ˆ).md\n",
      "loading: data/é“¶è”æ”¶å•æ‰£ç‡åˆ†ç±»ä¸æ ‡å‡†.md\n",
      "loading: data/é“¶è”æ¸…ç®—ä¸šåŠ¡ä½“ç³».md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:24,508 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:03<00:00,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: data/é“¶è¡Œäº‘é—ªä»˜ä¸€é”®æŸ¥å¡ä¸šåŠ¡è¯´æ˜.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:11:24,822 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:03<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶å¤§å°:11\n",
      "èŠ‚ç‚¹æ•°é‡:82\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨æ–¹æ³•\n",
    "documents_dir=  \"./data\"\n",
    "documents = load_documents_parallel(documents_dir, file_extractor, max_workers=1)\n",
    "\n",
    "cleaned_documents = [Document(text=clean_text(doc.text), metadata=doc.metadata) \n",
    "                     for doc in documents]\n",
    "\n",
    "# æ·»åŠ è¿™ä¸€æ­¥ï¼šæœ€å¤§é•¿åº¦100000ï¼Œå‰åé‡å 1000\n",
    "# cleaned_documents = preprocess_long_documents(\n",
    "#     cleaned_documents, \n",
    "#     max_length=100000, \n",
    "#     overlap=0\n",
    "# )\n",
    "documents = cleaned_documents\n",
    "\n",
    "log(f\"æ–‡ä»¶å¤§å°:{len(documents)}\")\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100, tokenizer=qwen_tokenizer.tokenize)  \n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "log(f\"èŠ‚ç‚¹æ•°é‡:{len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786076ab-5328-4eba-9b90-9e5669da92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48734949-0a3b-44d5-92b2-6b4d45100eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_summaries_to_json(nodes_list, file_path=\"nodes_summaries_temp.json\"):\n",
    "#     summaries_dict = {}\n",
    "#     for idx, node in enumerate(nodes_list):\n",
    "#         summaries_dict[str(idx)] = node.metadata.get(\"node_summary\", \"\")  # è·å–æ‘˜è¦ï¼Œè‹¥æ— åˆ™ä¸ºç©º\n",
    "    \n",
    "#     # ä¿å­˜åˆ° JSON\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "#     log(f\"èŠ‚ç‚¹æ‘˜è¦å·²ä¿å­˜åˆ° {file_path}\")\n",
    "\n",
    "# def load_summaries_to_nodes(nodes_list, file_path=\"nodes_summaries.json\"):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         summaries_dict = json.load(f)\n",
    "#     sorted_keys = sorted(summaries_dict.keys(), key=int)\n",
    "\n",
    "#     for key in sorted_keys:\n",
    "#         idx = int(key)\n",
    "#         if idx < len(nodes_list):\n",
    "#             nodes_list[idx].metadata[\"node_summary\"] = summaries_dict[key]\n",
    "#         else:\n",
    "#             log(f\"è­¦å‘Šï¼šç´¢å¼• {idx} è¶…å‡ºèŠ‚ç‚¹åˆ—è¡¨é•¿åº¦ï¼Œè·³è¿‡ã€‚\")\n",
    "    \n",
    "#     return nodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35dc24-2ae5-4cb1-8adf-14b5645bcd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da1464-2d9c-4c15-a3b0-cb2da61a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_summaries_to_json(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f633b2f-4bff-4ae3-9327-1d6a18ded997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# ============ ä¿å­˜ Nodes ============\n",
    "def save_nodes(nodes, save_dir=\"./saved_nodes\"):\n",
    "    \"\"\"ä¿å­˜èŠ‚ç‚¹æ•°æ®ï¼ˆæ”¯æŒpickleå’Œjsonä¸¤ç§æ ¼å¼ï¼‰\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # æ–¹æ³•1: ä½¿ç”¨ pickle ä¿å­˜å®Œæ•´èŠ‚ç‚¹å¯¹è±¡ï¼ˆæ¨èï¼‰\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "    log(f\"Nodeså·²ä¿å­˜åˆ°: {pickle_file}\")\n",
    "\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"åŠ è½½èŠ‚ç‚¹æ•°æ®\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°èŠ‚ç‚¹æ–‡ä»¶: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    log(f\"âœ… å·²åŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹\")\n",
    "    \n",
    "    # éªŒè¯æ•°æ®\n",
    "    log(f\"ğŸ“Š èŠ‚ç‚¹éªŒè¯:\")\n",
    "    log(f\"  - æ€»èŠ‚ç‚¹æ•°: {len(nodes)}\")\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62b589d3-18f7-4672-9653-2be5ba001a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodeså·²ä¿å­˜åˆ°: saved_nodes/nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "save_nodes(nodes, save_dir=\"./saved_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775b208d-7576-4475-89ad-5cafb8a9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=Settings.embed_model,\n",
    "#     node_parser=node_parser,\n",
    "#     store_nodes_override=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "transformations = [node_parser]\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model,\n",
    "    node_parser=node_parser,\n",
    "    transformations=transformations,\n",
    "    store_nodes_override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d8e8761-356e-4966-b632-003edb975da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: ./milvus_storage\n",
      "âœ… ç´¢å¼•ä¿¡æ¯å·²ä¿å­˜åˆ°: milvus_storage/index_info.json\n",
      "ğŸ“Š ç´¢å¼•ä¿¡æ¯: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/marathon_rag/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 82, 'index_type': 'VectorStoreIndex'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ ä¿å­˜ Milvus ç´¢å¼• ============\n",
    "def save_milvus_index(index, persist_dir=\"./milvus_storage\"):\n",
    "    \"\"\"ä¿å­˜Milvusç´¢å¼•ï¼ˆæŒä¹…åŒ–åˆ°æœ¬åœ°ï¼‰\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    persist_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # LlamaIndexä¼šè‡ªåŠ¨ä¿å­˜ç´¢å¼•ç»“æ„å’Œdocstore\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    \n",
    "    log(f\"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {persist_dir}\")\n",
    "    \n",
    "    # ä¿å­˜ç´¢å¼•å…ƒä¿¡æ¯\n",
    "    index_info = {\n",
    "        'collection_name': 'rag_collection',\n",
    "        'milvus_db_path': abs_db_path,\n",
    "        'embedding_dim': dimension,\n",
    "        'total_documents': len(index.docstore.docs),\n",
    "        'index_type': 'VectorStoreIndex'\n",
    "    }\n",
    "    \n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    with open(info_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(index_info, f, ensure_ascii=False, indent=2)\n",
    "    log(f\"âœ… ç´¢å¼•ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}\")\n",
    "    log(f\"ğŸ“Š ç´¢å¼•ä¿¡æ¯: {index_info}\")\n",
    "\n",
    "# ============ ä½¿ç”¨ç¤ºä¾‹ ============\n",
    "# ä¿å­˜ç´¢å¼•\n",
    "save_milvus_index(index, persist_dir=\"./milvus_storage\")\n",
    "\n",
    "# åŠ è½½ç´¢å¼•\n",
    "# index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17816c31-5fe8-4cb9-9237-f17cefd16c2c",
   "metadata": {},
   "source": [
    "### éç¬¬ä¸€æ¬¡è¿è¡Œ åŠ è½½æŒä¹…åŒ–è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103c2661-6d60-4e1c-a448-0b588aaaab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"åŠ è½½èŠ‚ç‚¹æ•°æ®\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°èŠ‚ç‚¹æ–‡ä»¶: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    log(f\"âœ… å·²åŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹\")\n",
    "    \n",
    "    # éªŒè¯æ•°æ®\n",
    "    log(f\"ğŸ“Š èŠ‚ç‚¹éªŒè¯:\")\n",
    "    log(f\"  - æ€»èŠ‚ç‚¹æ•°: {len(nodes)}\")\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155ebe6a-db42-4973-875e-7e484a786aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ åŠ è½½ Milvus ç´¢å¼• ============\n",
    "def load_milvus_index(persist_dir=\"./storage\", milvus_db_path=None):\n",
    "    \"\"\"åŠ è½½å·²ä¿å­˜çš„Milvusç´¢å¼•\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    \n",
    "    if not persist_path.exists():\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°ç´¢å¼•ç›®å½•: {persist_dir}\")\n",
    "    \n",
    "    # è¯»å–ç´¢å¼•ä¿¡æ¯\n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    if info_file.exists():\n",
    "        with open(info_file, 'r', encoding='utf-8') as f:\n",
    "            index_info = json.load(f)\n",
    "        log(f\"ğŸ“Š ç´¢å¼•ä¿¡æ¯: {index_info}\")\n",
    "        milvus_db_path = milvus_db_path or index_info.get('milvus_db_path')\n",
    "    \n",
    "    # é‡å»º Milvus vector store\n",
    "    milvus_vector_store = MilvusVectorStore(\n",
    "        uri=milvus_db_path,\n",
    "        collection_name=\"rag_collection\",\n",
    "        dim=dimension,\n",
    "        overwrite=False  # ä¸è¦†ç›–å·²æœ‰æ•°æ®\n",
    "    )\n",
    "    \n",
    "    # é‡å»º storage context\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=milvus_vector_store,\n",
    "        persist_dir=persist_dir\n",
    "    )\n",
    "    \n",
    "    # åŠ è½½ç´¢å¼•\n",
    "    index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        embed_model=Settings.embed_model\n",
    "    )\n",
    "    \n",
    "    log(f\"âœ… ç´¢å¼•å·²åŠ è½½\")\n",
    "    log(f\"  - æ–‡æ¡£æ•°é‡: {len(index.docstore.docs)}\")\n",
    "    \n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ae0603-9369-494a-a304-a44b37d76ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²åŠ è½½ 2493 ä¸ªèŠ‚ç‚¹\n",
      "ğŸ“Š èŠ‚ç‚¹éªŒè¯:\n",
      "  - æ€»èŠ‚ç‚¹æ•°: 2493\n",
      "ğŸ“Š ç´¢å¼•ä¿¡æ¯: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 2493, 'index_type': 'VectorStoreIndex'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-10-13 17:07:30,504 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/index_store.json.\n",
      "âœ… ç´¢å¼•å·²åŠ è½½\n",
      "  - æ–‡æ¡£æ•°é‡: 2493\n"
     ]
    }
   ],
   "source": [
    "nodes = load_nodes(save_dir=\"./saved_nodes\")\n",
    "index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n",
    "index.embed_model=Settings.embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cedb969-684f-4273-88a3-45e4c5edca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] åˆ†æ•°: 0.2065 | æ–‡ä»¶: None\n",
      "å†…å®¹: .............................................................................. 37 7.4 å…±äº«å…³ç³»ç®¡ç† ...........................................................................................................\n",
      "\n",
      "[2] åˆ†æ•°: 0.1932 | æ–‡ä»¶: None\n",
      "å†…å®¹: .......................................................... 19 5.1 äº¤æ˜“æŸ¥è¯¢ .................................................................................................................................\n",
      "\n",
      "[3] åˆ†æ•°: 0.1927 | æ–‡ä»¶: None\n",
      "å†…å®¹: ............................................................................................................ 40 10.1.3 æ“ä½œé¡µé¢ ............................................................................\n"
     ]
    }
   ],
   "source": [
    "# å•æ¬¡å¿«é€Ÿæ£€ç´¢\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "nodes_test = retriever.retrieve(\"è…¾è®¯æ¸¸æˆ ä¸‰è§’æ´²è¡ŒåŠ¨\")\n",
    "\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "for i, node in enumerate(nodes_test, 1):\n",
    "    log(f\"\\n[{i}] åˆ†æ•°: {node.score:.4f} | æ–‡ä»¶: {node.metadata.get('file_name')}\")\n",
    "    log(f\"å†…å®¹: {node.text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b2a35-74d4-496c-afed-46c7795044ea",
   "metadata": {},
   "source": [
    "## search and rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf4934f-e7ed-479a-9c08-f0d91be86b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-retrievers-bm25\n",
    "# !pip install llama-index-packs-fusion-retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d73f2f98-4f58-400c-8ba3-f7ca568dea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever,QueryFusionRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.fusion_retriever import HybridFusionRetrieverPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17faba70-2b48-4577-9cf0-682a6074d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨ä»£ç å¼€å¤´æ·»åŠ è¿™äº›è°ƒè¯•å‡½æ•°\n",
    "import json\n",
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "def print_retrieved_nodes(nodes: List[NodeWithScore], title=\"æ£€ç´¢åˆ°çš„èŠ‚ç‚¹\"):\n",
    "    \"\"\"æ‰“å°æ£€ç´¢åˆ°çš„èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯\"\"\"\n",
    "    log(f\"\\n{'='*80}\")\n",
    "    log(f\"{title} (å…± {len(nodes)} ä¸ª)\")\n",
    "    log(f\"{'='*80}\")\n",
    "    \n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        log(f\"\\n[èŠ‚ç‚¹ {i}]\")\n",
    "        log(f\"  åˆ†æ•°: {node.score:.4f}\")\n",
    "        log(f\"  èŠ‚ç‚¹ID: {node.node.node_id}\")\n",
    "        log(f\"  æ–‡ä»¶å: {node.node.metadata.get('file_name', 'N/A')}\")\n",
    "        \n",
    "        # å¦‚æœæ˜¯å­èŠ‚ç‚¹ï¼Œæ˜¾ç¤ºçˆ¶èŠ‚ç‚¹ä¿¡æ¯\n",
    "        if node.node.metadata.get('is_child_node'):\n",
    "            log(f\"  çˆ¶èŠ‚ç‚¹ID: {node.node.metadata.get('parent_node_id')}\")\n",
    "            log(f\"  å­èŠ‚ç‚¹ç´¢å¼•: {node.node.metadata.get('chunk_index')}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ–‡æœ¬å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰\n",
    "        log(f\"  å†…å®¹é¢„è§ˆ: {node.node.text[100]}\")\n",
    "        log(len(node.node.text))\n",
    "        \n",
    "        # æ˜¾ç¤ºå®Œæ•´metadata\n",
    "        log(f\"  Metadata: {json.dumps(node.node.metadata, ensure_ascii=False, indent=4)}\")\n",
    "    \n",
    "    log(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "def print_prompt_to_llm(query: str, context: str, template_name=\"\"):\n",
    "    \"\"\"æ‰“å°å‘é€ç»™LLMçš„å®Œæ•´prompt\"\"\"\n",
    "    log(f\"\\n{'='*80}\")\n",
    "    log(f\"å‘é€ç»™LLMçš„Prompt {template_name}\")\n",
    "    log(f\"{'='*80}\")\n",
    "    log(f\"\\nã€ç”¨æˆ·æŸ¥è¯¢ã€‘\\n{query}\")\n",
    "    log(f\"\\nã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘\\n{context}\")\n",
    "    log(f\"\\n{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cce8fc09-ed70-4785-b12b-04f0aeb0a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, MetadataMode\n",
    "from llama_index.core.bridge.pydantic import Field, PrivateAttr\n",
    "from llama_index.core.callbacks import CBEventType, EventPayload\n",
    "from typing import List, Optional, Any, Union\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class Qwen3Reranker(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    Qwen3-Reranker for reranking nodes based on relevance to query.\n",
    "    \n",
    "    Args:\n",
    "        model (str): Path to the Qwen3-Reranker model.\n",
    "        top_n (int): Number of nodes to return sorted by score. Defaults to 5.\n",
    "        device (str, optional): Device (like \"cuda\", \"cpu\") for computation. \n",
    "            If None, checks if a GPU can be used.\n",
    "        max_length (int): Maximum sequence length. Defaults to 8192.\n",
    "        instruction (str, optional): Custom instruction for reranking.\n",
    "        keep_retrieval_score (bool, optional): Whether to keep the retrieval score \n",
    "            in metadata. Defaults to False.\n",
    "        clear_cache_after_rerank (bool): Whether to clear GPU cache after reranking.\n",
    "            Defaults to True.\n",
    "        batch_size (int): Number of query-document pairs to process at once.\n",
    "            Defaults to 5. Lower values use less memory but may be slower.\n",
    "    \"\"\"\n",
    "    \n",
    "    model: str = Field(description=\"Path to Qwen3-Reranker model.\")\n",
    "    top_n: int = Field(default=5, description=\"Number of nodes to return sorted by score.\")\n",
    "    device: Optional[str] = Field(default=None, description=\"Device for computation.\")\n",
    "    max_length: int = Field(default=8192, description=\"Maximum sequence length.\")\n",
    "    instruction: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Custom instruction for reranking.\"\n",
    "    )\n",
    "    keep_retrieval_score: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether to keep the retrieval score in metadata.\",\n",
    "    )\n",
    "    clear_cache_after_rerank: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Whether to clear GPU cache after reranking.\",\n",
    "    )\n",
    "    batch_size: int = Field(\n",
    "        default=5,\n",
    "        description=\"Number of query-document pairs to process at once.\",\n",
    "    )\n",
    "    \n",
    "    # ç§æœ‰å±æ€§\n",
    "    _tokenizer: Any = PrivateAttr()\n",
    "    _model: Any = PrivateAttr()\n",
    "    _device: str = PrivateAttr()\n",
    "    _token_false_id: int = PrivateAttr()\n",
    "    _token_true_id: int = PrivateAttr()\n",
    "    _prefix: str = PrivateAttr()\n",
    "    _suffix: str = PrivateAttr()\n",
    "    _prefix_tokens: List[int] = PrivateAttr()\n",
    "    _suffix_tokens: List[int] = PrivateAttr()\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        top_n: int = 5,\n",
    "        device: Optional[str] = None,\n",
    "        max_length: int = 8192,\n",
    "        instruction: Optional[str] = None,\n",
    "        keep_retrieval_score: bool = False,\n",
    "        clear_cache_after_rerank: bool = True,\n",
    "        batch_size: int = 5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä¼ é€’æ‰€æœ‰ Field å±æ€§\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            top_n=top_n,\n",
    "            device=device,\n",
    "            max_length=max_length,\n",
    "            instruction=instruction,\n",
    "            keep_retrieval_score=keep_retrieval_score,\n",
    "            clear_cache_after_rerank=clear_cache_after_rerank,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # éªŒè¯ batch_size\n",
    "        if self.batch_size < 1:\n",
    "            raise ValueError(f\"batch_size must be >= 1, got {self.batch_size}\")\n",
    "        \n",
    "        # è®¾ç½®é»˜è®¤ instruction\n",
    "        if self.instruction is None:\n",
    "            self.instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "        \n",
    "        # æ¨æ–­è®¾å¤‡\n",
    "        if self.device is None:\n",
    "            self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self._device = self.device\n",
    "        \n",
    "        # åŠ è½½ tokenizer\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model, \n",
    "            padding_side='left'\n",
    "        )\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        try:\n",
    "            self._model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model,\n",
    "                # torch_dtype=torch.float16,  # é‡åŒ–\n",
    "                attn_implementation=\"flash_attention_2\"\n",
    "            ).to(self._device).eval()\n",
    "            log(\"âœ“ Using flash_attention_2\")\n",
    "        except Exception as e:\n",
    "            log(f\"âš  Flash attention not available, using default: {e}\")\n",
    "            self._model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model,\n",
    "                torch_dtype=torch.float16,\n",
    "            ).to(self._device).eval()\n",
    "        \n",
    "        # è·å– yes/no token ids\n",
    "        self._token_false_id = self._tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self._token_true_id = self._tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "        \n",
    "        # å®šä¹‰å‰ç¼€å’Œåç¼€\n",
    "        self._prefix = (\n",
    "            \"<|im_start|>system\\n\"\n",
    "            \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "            \"Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\"\n",
    "        )\n",
    "        self._suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self._prefix_tokens = self._tokenizer.encode(self._prefix, add_special_tokens=False)\n",
    "        self._suffix_tokens = self._tokenizer.encode(self._suffix, add_special_tokens=False)\n",
    "    \n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        \"\"\"è¿”å›ç±»åï¼Œç”¨äºåºåˆ—åŒ–\"\"\"\n",
    "        return \"Qwen3Reranker\"\n",
    "    \n",
    "    def _clear_gpu_cache(self):\n",
    "        \"\"\"æ¸…ç† GPU ç¼“å­˜å’Œ KV cache\"\"\"\n",
    "        if self._device.startswith(\"cuda\"):\n",
    "            # æ¸…ç†æ¨¡å‹çš„ KV cacheï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "            if hasattr(self._model, 'clear_cache'):\n",
    "                self._model.clear_cache()\n",
    "            \n",
    "            # æ¸…ç©º CUDA ç¼“å­˜\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # å¼ºåˆ¶åƒåœ¾å›æ”¶\n",
    "            gc.collect()\n",
    "            \n",
    "            # å¯é€‰ï¼šåŒæ­¥ CUDA æ“ä½œ\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    def _format_instruction(self, query: str, doc: str) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬\"\"\"\n",
    "        return f\"<Instruct>: {self.instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "    \n",
    "    def _process_inputs(self, pairs: List[str]):\n",
    "        \"\"\"å¤„ç†è¾“å…¥å¯¹\"\"\"\n",
    "        inputs = self._tokenizer(\n",
    "            pairs, \n",
    "            padding=False, \n",
    "            truncation='longest_first',\n",
    "            return_attention_mask=False, \n",
    "            max_length=self.max_length - len(self._prefix_tokens) - len(self._suffix_tokens)\n",
    "        )\n",
    "        \n",
    "        # æ·»åŠ å‰ç¼€å’Œåç¼€\n",
    "        for i, ele in enumerate(inputs['input_ids']):\n",
    "            inputs['input_ids'][i] = self._prefix_tokens + ele + self._suffix_tokens\n",
    "        \n",
    "        # å¡«å……\n",
    "        inputs = self._tokenizer.pad(\n",
    "            inputs, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self._device)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _compute_scores_batch(self, pairs: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        è®¡ç®—ä¸€æ‰¹ pairs çš„ç›¸å…³æ€§åˆ†æ•°\n",
    "        \n",
    "        Args:\n",
    "            pairs: æ ¼å¼åŒ–åçš„ query-document å¯¹åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            åˆ†æ•°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        inputs = self._process_inputs(pairs)\n",
    "        \n",
    "        try:\n",
    "            batch_scores = self._model(**inputs).logits[:, -1, :]\n",
    "            true_vector = batch_scores[:, self._token_true_id]\n",
    "            false_vector = batch_scores[:, self._token_false_id]\n",
    "            batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "            batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "            scores = batch_scores[:, 1].exp().tolist()\n",
    "            return scores\n",
    "        finally:\n",
    "            # ç«‹å³åˆ é™¤ inputs é‡Šæ”¾æ˜¾å­˜\n",
    "            del inputs\n",
    "            if self._device.startswith(\"cuda\"):\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    def _compute_scores(self, pairs: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        åˆ†æ‰¹è®¡ç®—æ‰€æœ‰ pairs çš„ç›¸å…³æ€§åˆ†æ•°\n",
    "        \n",
    "        Args:\n",
    "            pairs: æ ¼å¼åŒ–åçš„ query-document å¯¹åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            æ‰€æœ‰ pairs çš„åˆ†æ•°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        total_pairs = len(pairs)\n",
    "        \n",
    "        # åˆ†æ‰¹å¤„ç†\n",
    "        for i in range(0, total_pairs, self.batch_size):\n",
    "            batch_pairs = pairs[i:i + self.batch_size]\n",
    "            batch_scores = self._compute_scores_batch(batch_pairs)\n",
    "            all_scores.extend(batch_scores)\n",
    "            \n",
    "            # å¯é€‰ï¼šæ‰“å°è¿›åº¦\n",
    "            if total_pairs > self.batch_size:\n",
    "                processed = min(i + self.batch_size, total_pairs)\n",
    "                log(f\"Reranking progress: {processed}/{total_pairs} pairs processed\")\n",
    "        \n",
    "        return all_scores\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        é‡æ’åºèŠ‚ç‚¹ï¼ˆå¿…é¡»å®ç°çš„æŠ½è±¡æ–¹æ³•ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            nodes: å¾…é‡æ’åºçš„èŠ‚ç‚¹åˆ—è¡¨\n",
    "            query_bundle: æŸ¥è¯¢ä¿¡æ¯\n",
    "            \n",
    "        Returns:\n",
    "            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        if query_bundle is None:\n",
    "            raise ValueError(\"Missing query bundle in extra info.\")\n",
    "        \n",
    "        if len(nodes) == 0:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹\n",
    "            query_str = query_bundle.query_str\n",
    "            query_and_nodes = [\n",
    "                (\n",
    "                    query_str,\n",
    "                    node.node.get_content(metadata_mode=MetadataMode.EMBED),\n",
    "                )\n",
    "                for node in nodes\n",
    "            ]\n",
    "            \n",
    "            # æ ¼å¼åŒ–è¾“å…¥\n",
    "            pairs = [\n",
    "                self._format_instruction(query, doc) \n",
    "                for query, doc in query_and_nodes\n",
    "            ]\n",
    "            \n",
    "            # ä½¿ç”¨ callback manager è®°å½•äº‹ä»¶ï¼ˆå¯é€‰ä½†æ¨èï¼‰\n",
    "            with self.callback_manager.event(\n",
    "                CBEventType.RERANKING,\n",
    "                payload={\n",
    "                    EventPayload.NODES: nodes,\n",
    "                    EventPayload.MODEL_NAME: self.model,\n",
    "                    EventPayload.QUERY_STR: query_str,\n",
    "                    EventPayload.TOP_K: self.top_n,\n",
    "                },\n",
    "            ) as event:\n",
    "                # åˆ†æ‰¹å¤„ç†å¹¶è®¡ç®—åˆ†æ•°\n",
    "                scores = self._compute_scores(pairs)\n",
    "                \n",
    "                assert len(scores) == len(nodes), \\\n",
    "                    f\"Score count mismatch: got {len(scores)} scores for {len(nodes)} nodes\"\n",
    "                \n",
    "                # æ›´æ–°èŠ‚ç‚¹åˆ†æ•°\n",
    "                for node, score in zip(nodes, scores):\n",
    "                    if self.keep_retrieval_score:\n",
    "                        # ä¿ç•™åŸå§‹æ£€ç´¢åˆ†æ•°\n",
    "                        node.node.metadata[\"retrieval_score\"] = node.score\n",
    "                    node.score = float(score)\n",
    "                \n",
    "                # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å› top_n\n",
    "                new_nodes = sorted(\n",
    "                    nodes, \n",
    "                    key=lambda x: -x.score if x.score else 0\n",
    "                )[: self.top_n]\n",
    "                \n",
    "                # è®°å½•ç»“æœ\n",
    "                event.on_end(payload={EventPayload.NODES: new_nodes})\n",
    "            \n",
    "            return new_nodes\n",
    "        \n",
    "        finally:\n",
    "            # æ— è®ºæ˜¯å¦å‡ºé”™ï¼Œéƒ½æ¸…ç†ç¼“å­˜\n",
    "            if self.clear_cache_after_rerank:\n",
    "                self._clear_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48b72b94-9a54-4740-9b4c-e139ea8fe587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Flash attention not available, using default: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a832843bebb4fbabbe16c140086cfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reranker = Qwen3Reranker(\n",
    "    model=\"/root/autodl-tmp/Qwen3-Reranker-4B\",\n",
    "    top_n=5,\n",
    "    device=\"cuda\",\n",
    "    max_length=8192,\n",
    "    instruction=\"æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­æ–‡æ¡£æ˜¯å¦åŒ…å«ç›¸å…³ç­”æ¡ˆæˆ–ä¿¡æ¯\",\n",
    "    keep_retrieval_score=True,\n",
    "    clear_cache_after_rerank=True,\n",
    "    batch_size=5  # é»˜è®¤å€¼ï¼Œä¸€æ¬¡å¤„ç† 5 å¯¹\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "933289c0-cd96-4170-b131-649aeada250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc515c0de21a4a8797f12a5943bf9a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at autodl-tmp/Qwen3-Reranker-4B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "Pad token ID: 151669\n",
      "Model config pad_token_id: 151669\n"
     ]
    }
   ],
   "source": [
    "# reranker_model_path = \"autodl-tmp/Qwen3-Reranker-4B\"\n",
    "\n",
    "# reranker = SentenceTransformerRerank(\n",
    "#     model=reranker_model_path,\n",
    "#     top_n=5,\n",
    "#     device=\"cuda\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# cross_encoder = reranker._model\n",
    "# reranker_tokenizer = cross_encoder.tokenizer\n",
    "# reranker_model = cross_encoder.model\n",
    "\n",
    "# special_tokens = {'pad_token': '[PAD]'}\n",
    "# num_added_tokens = reranker_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# reranker_model.resize_token_embeddings(len(reranker_tokenizer))\n",
    "\n",
    "# reranker_tokenizer.pad_token = '[PAD]'\n",
    "# reranker_tokenizer.pad_token_id = reranker_tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "# reranker_model.config.pad_token_id = reranker_tokenizer.pad_token_id\n",
    "\n",
    "# log(f\"Pad token: {reranker_tokenizer.pad_token}\")\n",
    "# log(f\"Pad token ID: {reranker_tokenizer.pad_token_id}\")\n",
    "# log(f\"Model config pad_token_id: {reranker_model.config.pad_token_id}\")\n",
    "\n",
    "# custom_instruction = \"æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­æ–‡æ¡£æ˜¯å¦åŒ…å«ç›¸å…³ç­”æ¡ˆæˆ–ä¿¡æ¯\"  # ä¸­æ–‡åœºæ™¯\n",
    "# reranker = Qwen3Reranker(\n",
    "#     model_path=\"autodl-tmp/Qwen3-Reranker-4B\",\n",
    "#     top_n=5,\n",
    "#     device=\"cuda\",\n",
    "#     max_length=8192, #æœ€é•¿è¾“å…¥\n",
    "#     instruction=None,\n",
    "#     keep_retrieval_score=True  # å¦‚æœæƒ³ä¿ç•™åŸå§‹æ£€ç´¢åˆ†æ•°\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55e55514-7ff9-40fa-b4b0-f8a310914217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:13:50,832 - WARNING - The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "2025-10-16 14:13:50,856 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "def reranker_tokenize(text):\n",
    "    rerank_tokenizer = AutoTokenizer.from_pretrained(\"autodl-tmp/Qwen3-Reranker-8B\", padding_side='left')\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    tokens = rerank_tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes, \n",
    "    similarity_top_k=25,\n",
    "    tokenizer=reranker_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56bcf4b9-baeb-4364-9fd6-cc14f7b069b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = index.as_retriever(similarity_top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a729b8c7-8842-4eb9-b29a-9870ef23cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=50,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ab6a74b-1e68-4487-9b27-99f0d7a578e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker.top_n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c25c162-cde9-413e-b26a-eea12ef1637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template_str = (\n",
    "    \"ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä¸‹ï¼š\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œç”¨ä¸­æ–‡ç›´æ¥å›ç­”æŸ¥è¯¢ï¼Œç­”æ¡ˆåªèƒ½ä»ä¸Šä¸‹æ–‡çŸ¥è¯†ä¸­è·å–ï¼Œä¸è¦è‡ªå·±å‘æŒ¥ã€‚\\n\"\n",
    "    \"æŸ¥è¯¢ï¼š{query_str}\\n\"\n",
    "    \"å›ç­”ï¼š\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"åŸå§‹æŸ¥è¯¢æ˜¯ï¼š{query_str}\\n\"\n",
    "    \"æˆ‘ä»¬å·²æœ‰å›ç­”ï¼š{existing_answer}\\n\"\n",
    "    \"åŸºäºä»¥ä¸‹æ–°ä¸Šä¸‹æ–‡ï¼Œç”¨ä¸­æ–‡ç²¾ç‚¼ç°æœ‰å›ç­”ï¼Œé—®é¢˜çš„æ ¸å¿ƒå›ç­”è¦æ”¾åœ¨æœ€å‰è¾¹ï¼Œç„¶åæ˜¯è§£é‡Šï¼Œç¡®ä¿å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼š\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"ç²¾ç‚¼åçš„å›ç­”ï¼š\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18a7a043-1de9-41a8-9ce2-f51ee7157700",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=text_qa_template,\n",
    "    refine_template=refine_template,\n",
    "    response_mode=\"compact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e34b779-00c1-4b24-85dc-bfa4ff947a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LongContextReorder\n",
    "longcontextreorder=LongContextReorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48b14fdb-5b11-4de9-8d21-05918c0e989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[reranker,longcontextreorder]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18c73b8c-df5b-40b9-bc8c-ca15e31f3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking progress: 5/38 pairs processed\n",
      "Reranking progress: 10/38 pairs processed\n",
      "Reranking progress: 15/38 pairs processed\n",
      "Reranking progress: 20/38 pairs processed\n",
      "Reranking progress: 25/38 pairs processed\n",
      "Reranking progress: 30/38 pairs processed\n",
      "Reranking progress: 35/38 pairs processed\n",
      "Reranking progress: 38/38 pairs processed\n",
      "âœ“ è¯·æ±‚å·²ä¿å­˜åˆ°: llm_requests/requests_log.txt\n",
      "æ ¹æ®ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹çš„ç›¸å…³è§„å®šï¼Œé’ˆå¯¹è¯¥å®¢æˆ·ä½¿ç”¨è…¾è®¯å‘è¡Œçš„å•ä½å¡è´­ä¹°ä¸€ä»¶ä»·å€¼11ä¸‡å…ƒäººæ°‘å¸çš„è‰ºæœ¯å“çš„äº¤æ˜“ï¼Œå¯èƒ½å­˜åœ¨ä»¥ä¸‹è¿åâ€œæ”¯ä»˜ç»“ç®—åŠæ³•â€çš„æƒ…å†µï¼š\n",
      "\n",
      "1. **å•ä½å¡ç”¨äºå¤§é¢å•†å“äº¤æ˜“**ï¼š  \n",
      "   æ ¹æ®ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ç¬¬142æ¡ï¼šâ€œå•ä½å¡ä¸å¾—ç”¨äº10ä¸‡å…ƒä»¥ä¸Šçš„å•†å“äº¤æ˜“ã€åŠ³åŠ¡ä¾›åº”æ¬¾é¡¹çš„ç»“ç®—ã€‚â€  \n",
      "   è¯¥äº¤æ˜“é‡‘é¢ä¸º11ä¸‡å…ƒäººæ°‘å¸ï¼Œè¶…è¿‡äº†10ä¸‡å…ƒçš„é™é¢ï¼Œå› æ­¤**è¿åäº†è¯¥æ¡è§„å®š**ã€‚\n",
      "\n",
      "2. **å•ä½å¡èµ„é‡‘æ¥æºä¸ç”¨é€”ä¸ç¬¦**ï¼š  \n",
      "   æ ¹æ®ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ç¬¬137æ¡ï¼šâ€œå•ä½å¡å¸æˆ·çš„èµ„é‡‘ä¸€å¾‹ä»å…¶åŸºæœ¬å­˜æ¬¾å¸æˆ·è½¬å¸å­˜å…¥ï¼Œä¸å¾—äº¤å­˜ç°é‡‘ï¼Œä¸å¾—å°†é”€è´§æ”¶å…¥çš„æ¬¾é¡¹å­˜å…¥å…¶å¸æˆ·ã€‚â€  \n",
      "   è¯¥äº¤æ˜“æ˜¯é€šè¿‡å•ä½å¡æ”¯ä»˜ï¼Œä½†è´­ä¹°è‰ºæœ¯å“å±äºéç»è¥æ€§æ”¯å‡ºï¼Œä¸”æœªè¯´æ˜è¯¥èµ„é‡‘æ˜¯å¦ä»åŸºæœ¬å­˜æ¬¾è´¦æˆ·è½¬å¸è€Œæ¥ã€‚è‹¥è¯¥èµ„é‡‘æ¥æºäºéåŸºæœ¬è´¦æˆ·æˆ–ä¸ºç°é‡‘å­˜å…¥ï¼Œåˆ™è¿åäº†å•ä½å¡èµ„é‡‘æ¥æºçš„ç®¡ç†è§„å®šã€‚\n",
      "\n",
      "3. **å•ä½å¡ç”¨äºéç»è¥æ€§ç”¨é€”**ï¼š  \n",
      "   å•ä½å¡çš„ä½¿ç”¨åº”é™äºä¸å•ä½ç»è¥ç›¸å…³çš„ä¸šåŠ¡æ´»åŠ¨ã€‚è´­ä¹°è‰ºæœ¯å“ç”¨äºåŠå…¬å®¤è£…é¥°ï¼Œå±äºéç»è¥æ€§æ”¯å‡ºï¼Œä¸ç¬¦åˆå•ä½å¡çš„åˆè§„ä½¿ç”¨èŒƒå›´ï¼Œå¯èƒ½æ„æˆ**æ»¥ç”¨å•ä½å¡**ï¼Œè¿åã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹å…³äºå•ä½å¡ä½¿ç”¨èŒƒå›´çš„é™åˆ¶ã€‚\n",
      "\n",
      "ç»¼ä¸Šï¼Œè¯¥ç¬”äº¤æ˜“**è¿åäº†ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ç¬¬142æ¡å…³äºå•ä½å¡ä¸å¾—ç”¨äº10ä¸‡å…ƒä»¥ä¸Šå•†å“äº¤æ˜“çš„è§„å®š**ï¼Œå¹¶å¯èƒ½æ¶‰åŠå•ä½å¡èµ„é‡‘æ¥æºå’Œç”¨é€”çš„è¿è§„é—®é¢˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "æ‚¨æ˜¯ä¸€å®¶å¤§å‹å•†ä¸šé“¶è¡Œçš„é¦–å¸­åˆè§„å®˜ã€‚æ‚¨çš„ä¸€ä½å®¢æˆ·æ˜¯è…¾è®¯çš„ä¸€ä½é«˜ç®¡ï¼Œç”±äºå…¶å®¶åº­å…³ç³»ï¼Œä»–ä¹Ÿè¢«åˆ—ä¸ºâ€œå¤–å›½æ”¿è¦â€ã€‚åœ¨è…¾è®¯2025å¹´ç¬¬äºŒå­£åº¦è´¢æŠ¥å‘å¸ƒåçš„ä¸€å‘¨å†…ï¼Œä»–é€šè¿‡è´µè¡Œè¿›è¡Œäº†ä»¥ä¸‹äº¤æ˜“ï¼š\n",
    "\n",
    "ä»–ä½¿ç”¨è…¾è®¯å‘è¡Œçš„å•ä½å¡è´­ä¹°äº†ä¸€ä»¶ä»·å€¼11ä¸‡å…ƒäººæ°‘å¸çš„è‰ºæœ¯å“ï¼Œæ‘†æ”¾åœ¨åŠå…¬å®¤ã€‚\n",
    "\n",
    "ä»–å°†8ä¸‡å…ƒäººæ°‘å¸ç°é‡‘å­˜å…¥ä¸ªäººè´¦æˆ·ï¼Œå¹¶æ³¨æ˜è¿™ç¬”èµ„é‡‘æ¥è‡ªä¸ªäººè‚¡æ¯ã€‚\n",
    "\n",
    "ä»–ä½œä¸ºä»˜æ¬¾äººç­¾ç½²äº†ä¸€å¼ é‡‘é¢ä¸º600ä¸‡å…ƒäººæ°‘å¸çš„å•†ä¸šæ‰¿å…‘æ±‡ç¥¨ï¼Œä»˜æ¬¾æœŸé™ä¸º90å¤©ã€‚è¯¥è‰æ¡ˆæ—¨åœ¨ä¸ºä¸€å®¶3Dæ‰“å°å…¬å¸æä¾›æ–°çš„èèµ„ï¼Œè¯¥å…¬å¸å°†ä½¿ç”¨è…¾è®¯çš„â€œæ··å…ƒ3Dæ¨¡å‹â€äººå·¥æ™ºèƒ½æœåŠ¡ï¼Œè¯¥æœåŠ¡åœ¨æœ€è¿‘çš„è´¢æŠ¥ä¸­è¢«é‡ç‚¹æåŠã€‚\n",
    "\n",
    "æ‚¨çš„ä»»åŠ¡ï¼š\n",
    "\n",
    "å¯¹äºè¿™ç¬¬ä¸€ç¬”äº¤æ˜“ï¼Œè¯·åˆ†åˆ«æ‰¾å‡ºä»»ä½•å¯èƒ½è¿åâ€œæ”¯ä»˜ç»“ç®—åŠæ³•â€çš„æƒ…å†µ\"\"\"\n",
    "response = query_engine.query(query)\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb7e17f9-3478-45a2-bca6-3734af8d0f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ã€æ­¥éª¤1ï¼šæ··åˆæ£€ç´¢ã€‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ ã€æ­¥éª¤2ï¼šReranké‡æ’åºã€‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking progress: 5/38 pairs processed\n",
      "Reranking progress: 10/38 pairs processed\n",
      "Reranking progress: 15/38 pairs processed\n",
      "Reranking progress: 20/38 pairs processed\n",
      "Reranking progress: 25/38 pairs processed\n",
      "Reranking progress: 30/38 pairs processed\n",
      "Reranking progress: 35/38 pairs processed\n",
      "Reranking progress: 38/38 pairs processed\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "æ‚¨æ˜¯ä¸€å®¶å¤§å‹å•†ä¸šé“¶è¡Œçš„é¦–å¸­åˆè§„å®˜ã€‚æ‚¨çš„ä¸€ä½å®¢æˆ·æ˜¯è…¾è®¯çš„ä¸€ä½é«˜ç®¡ï¼Œç”±äºå…¶å®¶åº­å…³ç³»ï¼Œä»–ä¹Ÿè¢«åˆ—ä¸ºâ€œå¤–å›½æ”¿è¦â€ã€‚åœ¨è…¾è®¯2025å¹´ç¬¬äºŒå­£åº¦è´¢æŠ¥å‘å¸ƒåçš„ä¸€å‘¨å†…ï¼Œä»–é€šè¿‡è´µè¡Œè¿›è¡Œäº†ä»¥ä¸‹äº¤æ˜“ï¼š\n",
    "\n",
    "ä»–ä½¿ç”¨è…¾è®¯å‘è¡Œçš„å•ä½å¡è´­ä¹°äº†ä¸€ä»¶ä»·å€¼11ä¸‡å…ƒäººæ°‘å¸çš„è‰ºæœ¯å“ï¼Œæ‘†æ”¾åœ¨åŠå…¬å®¤ã€‚\n",
    "\n",
    "ä»–å°†8ä¸‡å…ƒäººæ°‘å¸ç°é‡‘å­˜å…¥ä¸ªäººè´¦æˆ·ï¼Œå¹¶æ³¨æ˜è¿™ç¬”èµ„é‡‘æ¥è‡ªä¸ªäººè‚¡æ¯ã€‚\n",
    "\n",
    "ä»–ä½œä¸ºä»˜æ¬¾äººç­¾ç½²äº†ä¸€å¼ é‡‘é¢ä¸º600ä¸‡å…ƒäººæ°‘å¸çš„å•†ä¸šæ‰¿å…‘æ±‡ç¥¨ï¼Œä»˜æ¬¾æœŸé™ä¸º90å¤©ã€‚è¯¥è‰æ¡ˆæ—¨åœ¨ä¸ºä¸€å®¶3Dæ‰“å°å…¬å¸æä¾›æ–°çš„èèµ„ï¼Œè¯¥å…¬å¸å°†ä½¿ç”¨è…¾è®¯çš„â€œæ··å…ƒ3Dæ¨¡å‹â€äººå·¥æ™ºèƒ½æœåŠ¡ï¼Œè¯¥æœåŠ¡åœ¨æœ€è¿‘çš„è´¢æŠ¥ä¸­è¢«é‡ç‚¹æåŠã€‚\n",
    "\n",
    "æ‚¨çš„ä»»åŠ¡ï¼š\n",
    "\n",
    "ä»…æ ¹æ®æä¾›çš„æ–‡ä»¶ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ã€‚\n",
    "\n",
    "å¯¹äºè¿™ç¬¬ä¸€ç¬”äº¤æ˜“ï¼Œè¯·åˆ†åˆ«æ‰¾å‡ºä»»ä½•å¯èƒ½è¿åâ€œæ”¯ä»˜ç»“ç®—åŠæ³•â€çš„æƒ…å†µã€‚è¯·å¼•ç”¨æ–‡ä»¶ä¸­çš„å…·ä½“æ¡æ¬¾ç¼–å·æ¥æ”¯æŒæ‚¨çš„å‘ç°ã€‚\"\"\"\n",
    "\n",
    "# # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨retrieveræŸ¥çœ‹æ£€ç´¢ç»“æœ\n",
    "log(\"\\nğŸ” ã€æ­¥éª¤1ï¼šæ··åˆæ£€ç´¢ã€‘\")\n",
    "retrieved_nodes = hybrid_retriever.retrieve(query)\n",
    "# print_retrieved_nodes(retrieved_nodes, \"æ··åˆæ£€ç´¢ç»“æœ\")\n",
    "\n",
    "# # æ–¹æ³•2ï¼šæŸ¥çœ‹rerankåçš„ç»“æœ\n",
    "log(\"\\nğŸ¯ ã€æ­¥éª¤2ï¼šReranké‡æ’åºã€‘\")\n",
    "from llama_index.core.schema import QueryBundle\n",
    "query_bundle = QueryBundle(query_str=query)\n",
    "reranked_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "# print_retrieved_nodes(reranked_nodes, \"Rerankåçš„èŠ‚ç‚¹\")\n",
    "\n",
    "# # # æ–¹æ³•3ï¼šæŸ¥çœ‹æœ€ç»ˆå‘é€ç»™LLMçš„å†…å®¹\n",
    "# log(\"\\nğŸ“ ã€æ­¥éª¤3ï¼šç”Ÿæˆå›ç­”ã€‘\")\n",
    "# # æ‰‹åŠ¨æ„å»ºcontextæ¥æŸ¥çœ‹\n",
    "# context_str = \"\\n\\n\".join([node.node.get_content() for node in reranked_nodes])\n",
    "# print_prompt_to_llm(query, context_str, \"(text_qa_template)\")\n",
    "\n",
    "# # æ‰§è¡ŒæŸ¥è¯¢\n",
    "# response = query_engine.query(query)\n",
    "# log(f\"\\nâœ… ã€æœ€ç»ˆå›ç­”ã€‘\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06741b50-ba2e-4380-8e2d-4268b1703e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import List, Optional, Dict\n",
    "import copy\n",
    "\n",
    "# ==================== 1. èŠ‚ç‚¹åˆ†å‰²å™¨ ====================\n",
    "class NodeSplitter:\n",
    "    \"\"\"å°†é•¿èŠ‚ç‚¹åˆ†å‰²æˆå¤šä¸ªå­èŠ‚ç‚¹,ä¿æŒçˆ¶å­å…³ç³»\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, overlap_ratio: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: å­èŠ‚ç‚¹çš„ç›®æ ‡é•¿åº¦\n",
    "            overlap_ratio: é‡å æ¯”ä¾‹ (0.1 è¡¨ç¤º 10%)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_size = int(chunk_size * overlap_ratio)\n",
    "        \n",
    "    def split_node(self, node: NodeWithScore, parent_id: str = None) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        å°†å•ä¸ªèŠ‚ç‚¹åˆ†å‰²æˆå¤šä¸ªå­èŠ‚ç‚¹\n",
    "        \n",
    "        Args:\n",
    "            node: åŸå§‹èŠ‚ç‚¹\n",
    "            parent_id: çˆ¶èŠ‚ç‚¹ID (å¦‚æœä¸ºNone,ä½¿ç”¨node.node.node_id)\n",
    "            \n",
    "        Returns:\n",
    "            å­èŠ‚ç‚¹åˆ—è¡¨,æ¯ä¸ªå­èŠ‚ç‚¹éƒ½ä¿ç•™çˆ¶èŠ‚ç‚¹å¼•ç”¨\n",
    "        \"\"\"\n",
    "        text = node.node.text\n",
    "        text_length = len(text)\n",
    "        \n",
    "        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºchunk_size,ç›´æ¥è¿”å›åŸèŠ‚ç‚¹\n",
    "        if text_length <= self.chunk_size:\n",
    "            # æ·»åŠ çˆ¶èŠ‚ç‚¹IDåˆ°metadata\n",
    "            node.node.metadata['parent_node_id'] = parent_id or node.node.node_id\n",
    "            node.node.metadata['is_child_node'] = False\n",
    "            return [node]\n",
    "        \n",
    "        parent_node_id = parent_id or node.node.node_id\n",
    "        child_nodes = []\n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start < text_length:\n",
    "            end = min(start + self.chunk_size, text_length)\n",
    "            chunk_text = text[start:end]\n",
    "            \n",
    "            # åˆ›å»ºå­èŠ‚ç‚¹\n",
    "            child_node = TextNode(\n",
    "                text=chunk_text,\n",
    "                metadata={\n",
    "                    **node.node.metadata,  # ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„metadata\n",
    "                    'parent_node_id': parent_node_id,\n",
    "                    'chunk_index': chunk_index,\n",
    "                    'is_child_node': True,\n",
    "                    'parent_text_length': text_length,\n",
    "                    'chunk_start': start,\n",
    "                    'chunk_end': end\n",
    "                },\n",
    "                excluded_embed_metadata_keys=node.node.excluded_embed_metadata_keys,\n",
    "                excluded_llm_metadata_keys=node.node.excluded_llm_metadata_keys,\n",
    "            )\n",
    "            \n",
    "            # ä¿æŒåŸå§‹è¯„åˆ†\n",
    "            child_node_with_score = NodeWithScore(\n",
    "                node=child_node,\n",
    "                score=node.score\n",
    "            )\n",
    "            \n",
    "            child_nodes.append(child_node_with_score)\n",
    "            \n",
    "            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·ç‚¹ (å¸¦é‡å )\n",
    "            start += (self.chunk_size - self.overlap_size)\n",
    "            chunk_index += 1\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def split_nodes(self, nodes: List[NodeWithScore]) -> tuple[List[NodeWithScore], Dict[str, NodeWithScore]]:\n",
    "        \"\"\"\n",
    "        æ‰¹é‡åˆ†å‰²èŠ‚ç‚¹\n",
    "        \n",
    "        Returns:\n",
    "            (å­èŠ‚ç‚¹åˆ—è¡¨, çˆ¶èŠ‚ç‚¹æ˜ å°„å­—å…¸)\n",
    "        \"\"\"\n",
    "        all_child_nodes = []\n",
    "        parent_node_map = {}  # parent_node_id -> åŸå§‹çˆ¶èŠ‚ç‚¹\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.node_id\n",
    "            parent_node_map[parent_id] = node  # ä¿å­˜åŸå§‹çˆ¶èŠ‚ç‚¹\n",
    "            \n",
    "            child_nodes = self.split_node(node, parent_id)\n",
    "            all_child_nodes.extend(child_nodes)\n",
    "        \n",
    "        return all_child_nodes, parent_node_map\n",
    "\n",
    "\n",
    "# ==================== 2. å­èŠ‚ç‚¹åˆ°çˆ¶èŠ‚ç‚¹çš„åå¤„ç†å™¨ ====================\n",
    "class ChildToParentPostprocessor(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    å°†rerankåçš„å­èŠ‚ç‚¹è¿˜åŸä¸ºçˆ¶èŠ‚ç‚¹\n",
    "    ç­–ç•¥: å¦‚æœå¤šä¸ªå­èŠ‚ç‚¹æ¥è‡ªåŒä¸€çˆ¶èŠ‚ç‚¹,å–æœ€é«˜åˆ†çš„å­èŠ‚ç‚¹åˆ†æ•°ä½œä¸ºçˆ¶èŠ‚ç‚¹åˆ†æ•°\n",
    "    \"\"\"\n",
    "    \n",
    "    # ä½¿ç”¨ Pydantic çš„æ–¹å¼å£°æ˜å­—æ®µ\n",
    "    parent_node_map: Dict[str, Any] = {}\n",
    "    keep_top_k: int = 5\n",
    "    \n",
    "    def __init__(self, parent_node_map: Dict[str, NodeWithScore], keep_top_k: int = 5, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parent_node_map: çˆ¶èŠ‚ç‚¹IDåˆ°çˆ¶èŠ‚ç‚¹çš„æ˜ å°„\n",
    "            keep_top_k: æœ€ç»ˆä¿ç•™çš„çˆ¶èŠ‚ç‚¹æ•°é‡\n",
    "        \"\"\"\n",
    "        # ä½¿ç”¨ Pydantic çš„åˆå§‹åŒ–æ–¹å¼\n",
    "        super().__init__(\n",
    "            parent_node_map=parent_node_map,\n",
    "            keep_top_k=keep_top_k,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self, \n",
    "        nodes: List[NodeWithScore], \n",
    "        query_bundle: Optional[QueryBundle] = None\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        å°†å­èŠ‚ç‚¹è¿˜åŸä¸ºçˆ¶èŠ‚ç‚¹\n",
    "        \"\"\"\n",
    "        # æŒ‰çˆ¶èŠ‚ç‚¹IDåˆ†ç»„,è®°å½•æ¯ä¸ªçˆ¶èŠ‚ç‚¹çš„æœ€é«˜åˆ†æ•°\n",
    "        parent_scores: Dict[str, float] = {}\n",
    "        parent_child_nodes: Dict[str, List[NodeWithScore]] = {}\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.metadata.get('parent_node_id')\n",
    "            \n",
    "            if not parent_id:\n",
    "                # å¦‚æœæ²¡æœ‰çˆ¶èŠ‚ç‚¹ID,è¯´æ˜æ˜¯åŸå§‹èŠ‚ç‚¹,ç›´æ¥ä¿ç•™\n",
    "                parent_scores[node.node.node_id] = node.score\n",
    "                parent_child_nodes[node.node.node_id] = [node]\n",
    "                continue\n",
    "            \n",
    "            # è®°å½•æœ€é«˜åˆ†æ•°\n",
    "            if parent_id not in parent_scores:\n",
    "                parent_scores[parent_id] = node.score\n",
    "                parent_child_nodes[parent_id] = [node]\n",
    "            else:\n",
    "                # å–æœ€é«˜åˆ†\n",
    "                parent_scores[parent_id] = max(parent_scores[parent_id], node.score)\n",
    "                parent_child_nodes[parent_id].append(node)\n",
    "        \n",
    "        # æ„å»ºçˆ¶èŠ‚ç‚¹åˆ—è¡¨\n",
    "        parent_nodes = []\n",
    "        for parent_id, score in parent_scores.items():\n",
    "            if parent_id in self.parent_node_map:\n",
    "                # ä½¿ç”¨ä¿å­˜çš„åŸå§‹çˆ¶èŠ‚ç‚¹\n",
    "                parent_node = copy.deepcopy(self.parent_node_map[parent_id])\n",
    "                parent_node.score = score\n",
    "                \n",
    "                # å¯é€‰: åœ¨metadataä¸­è®°å½•åŒ¹é…çš„å­èŠ‚ç‚¹ä¿¡æ¯\n",
    "                child_info = [\n",
    "                    {\n",
    "                        'chunk_index': n.node.metadata.get('chunk_index'),\n",
    "                        'score': n.score,\n",
    "                        'text_preview': n.node.text[:100]\n",
    "                    }\n",
    "                    for n in parent_child_nodes[parent_id]\n",
    "                ]\n",
    "                parent_node.node.metadata['matched_children'] = child_info\n",
    "                \n",
    "                parent_nodes.append(parent_node)\n",
    "            else:\n",
    "                # å¦‚æœæ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹,ä½¿ç”¨ç¬¬ä¸€ä¸ªå­èŠ‚ç‚¹(ä¸åº”è¯¥å‘ç”Ÿ)\n",
    "                log(f\"è­¦å‘Š: æ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ {parent_id}, ä½¿ç”¨å­èŠ‚ç‚¹ä»£æ›¿\")\n",
    "                parent_nodes.append(parent_child_nodes[parent_id][0])\n",
    "        \n",
    "        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k\n",
    "        parent_nodes.sort(key=lambda x: x.score, reverse=True)\n",
    "        return parent_nodes[:self.keep_top_k]\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # å…è®¸ä»»æ„ç±»å‹\n",
    "\n",
    "# ==================== 3. è‡ªå®šä¹‰æ£€ç´¢å™¨åŒ…è£…å™¨ ====================\n",
    "class SplitNodeRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    åŒ…è£…åŸå§‹æ£€ç´¢å™¨,è‡ªåŠ¨å¤„ç†èŠ‚ç‚¹åˆ†å‰²\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_retriever: BaseRetriever,\n",
    "        chunk_size: int = 512,\n",
    "        overlap_ratio: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_retriever: åŸå§‹æ··åˆæ£€ç´¢å™¨\n",
    "            chunk_size: å­èŠ‚ç‚¹å¤§å°\n",
    "            overlap_ratio: é‡å æ¯”ä¾‹\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_retriever = base_retriever\n",
    "        self.node_splitter = NodeSplitter(chunk_size, overlap_ratio)\n",
    "        self.parent_node_map = {}\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        æ£€ç´¢å¹¶åˆ†å‰²èŠ‚ç‚¹\n",
    "        \"\"\"\n",
    "        # 1. ä½¿ç”¨åŸå§‹æ£€ç´¢å™¨æ£€ç´¢\n",
    "        nodes = self.base_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # 2. åˆ†å‰²èŠ‚ç‚¹\n",
    "        child_nodes, self.parent_node_map = self.node_splitter.split_nodes(nodes)\n",
    "        \n",
    "        log(f\"åŸå§‹èŠ‚ç‚¹æ•°: {len(nodes)}, åˆ†å‰²åå­èŠ‚ç‚¹æ•°: {len(child_nodes)}\")\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def get_parent_node_map(self) -> Dict[str, NodeWithScore]:\n",
    "        \"\"\"è·å–çˆ¶èŠ‚ç‚¹æ˜ å°„,ä¾›åå¤„ç†å™¨ä½¿ç”¨\"\"\"\n",
    "        return self.parent_node_map\n",
    "\n",
    "\n",
    "def create_parent_postprocessor(retriever: SplitNodeRetriever, keep_top_k: int = 5):\n",
    "    \"\"\"åŠ¨æ€åˆ›å»ºçˆ¶èŠ‚ç‚¹åå¤„ç†å™¨\"\"\"\n",
    "    return ChildToParentPostprocessor(\n",
    "        parent_node_map=retriever.get_parent_node_map(),\n",
    "        keep_top_k=keep_top_k\n",
    "    )\n",
    "\n",
    "\n",
    "class DynamicQueryEngine:\n",
    "    \"\"\"æ”¯æŒåŠ¨æ€åå¤„ç†å™¨çš„æŸ¥è¯¢å¼•æ“\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        retriever, \n",
    "        response_synthesizer, \n",
    "        reranker, \n",
    "        keep_top_k=5,\n",
    "        use_parent_nodes=True,\n",
    "        reorder=None\n",
    "    ):\n",
    "        self.retriever = retriever\n",
    "        self.response_synthesizer = response_synthesizer\n",
    "        self.reranker = reranker\n",
    "        self.keep_top_k = keep_top_k\n",
    "        self.use_parent_nodes = use_parent_nodes\n",
    "        self.reorder = reorder\n",
    "\n",
    "    def longcontext_postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore]\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Postprocess nodes.\"\"\"\n",
    "        reordered_nodes: List[NodeWithScore] = []\n",
    "        ordered_nodes: List[NodeWithScore] = sorted(\n",
    "            nodes, key=lambda x: x.score if x.score is not None else 0\n",
    "        )\n",
    "        for i, node in enumerate(ordered_nodes):\n",
    "            if i % 2 == 0:\n",
    "                reordered_nodes.insert(0, node)\n",
    "            else:\n",
    "                reordered_nodes.append(node)\n",
    "        return reordered_nodes\n",
    "    \n",
    "    def query(self, query_str: str):\n",
    "        from llama_index.core.schema import QueryBundle\n",
    "        \n",
    "        # è®°å½•æ€»å¼€å§‹æ—¶é—´\n",
    "        total_start = time.time()\n",
    "        timing_stats: Dict[str, float] = {}\n",
    "        \n",
    "        # 1. æ£€ç´¢ (è‡ªåŠ¨åˆ†å‰²èŠ‚ç‚¹)\n",
    "        retrieval_start = time.time()\n",
    "        query_bundle = QueryBundle(query_str=query_str)\n",
    "        nodes = self.retriever.retrieve(query_bundle)\n",
    "        timing_stats['æ£€ç´¢'] = time.time() - retrieval_start\n",
    "        \n",
    "        # 2. Rerankå­èŠ‚ç‚¹\n",
    "        rerank_start = time.time()\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(nodes, query_bundle)\n",
    "        timing_stats['Rerank'] = time.time() - rerank_start\n",
    "        \n",
    "        # 3. æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦è¿˜åŸçˆ¶èŠ‚ç‚¹\n",
    "        parent_start = time.time()\n",
    "        if self.use_parent_nodes:\n",
    "            parent_postprocessor = create_parent_postprocessor(\n",
    "                self.retriever, \n",
    "                keep_top_k=self.keep_top_k\n",
    "            )\n",
    "            final_nodes = parent_postprocessor.postprocess_nodes(reranked_nodes, query_bundle)\n",
    "            timing_stats['è¿˜åŸçˆ¶èŠ‚ç‚¹'] = time.time() - parent_start\n",
    "        else:\n",
    "            final_nodes = reranked_nodes[:self.keep_top_k]\n",
    "            timing_stats['æˆªå–èŠ‚ç‚¹'] = time.time() - parent_start\n",
    "        \n",
    "        # 4. Reorder (å¦‚æœå¯ç”¨)\n",
    "        if self.reorder:\n",
    "            reorder_start = time.time()\n",
    "            final_nodes = self.longcontext_postprocess_nodes(final_nodes)\n",
    "            timing_stats['Reorder'] = time.time() - reorder_start\n",
    "        \n",
    "        # 5. ç”Ÿæˆå›ç­”\n",
    "        synthesis_start = time.time()\n",
    "        response = self.response_synthesizer.synthesize(\n",
    "            query=query_str,\n",
    "            nodes=final_nodes\n",
    "        )\n",
    "        timing_stats['ç”Ÿæˆå›ç­”'] = time.time() - synthesis_start\n",
    "        \n",
    "        # è®¡ç®—æ€»è€—æ—¶\n",
    "        timing_stats['æ€»è€—æ—¶'] = time.time() - total_start\n",
    "        \n",
    "        # ç®€æ´çš„è€—æ—¶è¾“å‡º\n",
    "        # log(f\"æ£€ç´¢: {timing_stats['æ£€ç´¢']:.2f}s | Rerank: {timing_stats['Rerank']:.2f}s | ç”Ÿæˆ: {timing_stats['ç”Ÿæˆå›ç­”']:.2f}s | æ€»è®¡: {timing_stats['æ€»è€—æ—¶']:.2f}s\")\n",
    "\n",
    "            # æ‰“å°è€—æ—¶ç»Ÿè®¡\n",
    "        log(\"\\n\" + \"=\"*50)\n",
    "        log(\"â±ï¸  è€—æ—¶ç»Ÿè®¡:\")\n",
    "        log(\"=\"*50)\n",
    "        for step, duration in timing_stats.items():\n",
    "            if step != 'æ€»è€—æ—¶':\n",
    "                percentage = (duration / timing_stats['æ€»è€—æ—¶']) * 100\n",
    "                log(f\"{step:12s}: {duration:6.3f}ç§’ ({percentage:5.1f}%)\")\n",
    "        log(\"-\"*50)\n",
    "        log(f\"{'æ€»è€—æ—¶':12s}: {timing_stats['æ€»è€—æ—¶']:6.3f}ç§’ (100.0%)\")\n",
    "        log(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236213d2-b889-4a13-89ce-8d5939dce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_retriever = SplitNodeRetriever(\n",
    "    base_retriever=hybrid_retriever,\n",
    "    chunk_size=256,      # åˆ†å‰²ä¸º512é•¿åº¦ (æˆ–256)\n",
    "    overlap_ratio=0    # 0%é‡å \n",
    ")\n",
    "\n",
    "\n",
    "reranker.top_n=10\n",
    "reranker.batch_size=10\n",
    "\n",
    "dynamic_query_engine = DynamicQueryEngine(\n",
    "    retriever=split_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    reranker=reranker,\n",
    "    keep_top_k=10,\n",
    "    use_parent_nodes=False,  # ğŸ”¥ ç›´æ¥ç”¨top5å­èŠ‚ç‚¹\n",
    "    reorder=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4879465-574b-4ca0-b7e6-dcd6c1a67aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "åœ¨äº‘é—ªä»˜ä¸šåŠ¡ä¸­ï¼Œæ€»è¡Œç½‘ç»œé‡‘èäº‹ä¸šéƒ¨çš„ä¸»è¦èŒè´£æ˜¯ä»€ä¹ˆ\"\"\"\n",
    "response = dynamic_query_engine.query(query)\n",
    "log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93e7cb-d71f-4b30-91b1-b8e13a457585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
